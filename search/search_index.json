{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"# Introduction to Snakemake  ![image](./theme_images/sm_logo) <p>Automate Your Workflow With Snakemake</p> <p>Are you working with big data?</p> <p>Do you need to pass your data through various software?</p> <p>Oh wait! Have I updated this output file?</p> <p>If you\u2019ve ever been in this situation, you would know that it can become quite difficult to maintain consistency and accuracy.</p> <p>The more manual steps we execute, the more human errors that are inevitably introduced into our analysis - hampering accuracy and reproducibility.</p> <p>Sit back and let the machines do their magic.</p> <p>Workflow languages automate your data analysis workflow. They also ensure that all your analysis logs are captured in an organized fashion, explicitly outline the software used, capture the input and output files at each step and even allow you to restart the pipeline from where it errored out. The process ensures higher productivity and decreases loss of resources re-running your workflow from the start. Additionally, when your data inevitably becomes big data, workflow languages allow you to easily scale up - meaning, you can move your analysis to a high performance cluster (HPC) without stress!</p> <p>In this hands-on workshop,We will guide you through an introduction to Snakemake, a workflow language with its basis in the popular programming language, Python. Attendees can expect to learn:</p> <ul> <li>The benefits of using Snakemake or other workflow languages,</li> <li>How to create a workflow to organize your computations, and</li> <li>How an HPC scheduler (such as Slurm) fits into your workflow</li> </ul> <p>Attribution notice</p> <ul> <li>Material used in this workshop is based on the following repository<ul> <li>Author     : Leah Kemp (ESR)</li> <li>Repository : https://leahkemp.github.io/RezBaz2020_snakemake_workshop/</li> </ul> </li> </ul> <p>Workshop</p> <p>Workshop sections:</p> <ul> <li>01 - Introduction</li> <li>02 - Setup</li> <li>03 - Create a basic workflow</li> <li>04 - Leveling up your workflow</li> <li>05 - We want more!</li> </ul>"},{"location":"workshop_material/01_introduction/","title":"01 - Introduction","text":""},{"location":"workshop_material/01_introduction/#01-introduction","title":"01 - Introduction","text":""},{"location":"workshop_material/01_introduction/#this-workshop","title":"This workshop","text":"<p>Note</p> <p>This workshop has been adapted for use in CMSE 890-602 Reproducible Computational Workflows. It was previously designed by Leah Kemp and NeSI and can be found here https://github.com/nesi/snakemake_workshop</p> <p>This workshop is designed with someone who had some familiarity with the command line. However, I've tried to make it as accessible as possible to anyone who wants to learn Snakemake.</p> <p>Throughout this workshop, I'll be indicating the code to remove and the code to insert (relative to the previous step) with the following:</p> <pre><code>- Remove this line of code\n+ Add this line of code\n</code></pre> <p>However, the actual <code>+</code> and <code>-</code> symbols should not be included in your own code</p> <p>At each section of the workshop you can find a drop down box under \"Current snakefile:\" that will contain the main Snakefile that comprises the pipeline as a plain text file to copy and paste from if you need to catch up.</p> <p>Back to homepage</p>"},{"location":"workshop_material/02_setup/","title":"02 - Setup","text":""},{"location":"workshop_material/02_setup/#02-setup","title":"02 - Setup","text":""},{"location":"workshop_material/02_setup/#install-miniforge","title":"Install Miniforge","text":"<p>For this workshop, we will analyse our data using various software. However, the only software we will need to manually install is Miniforge.</p>"},{"location":"workshop_material/02_setup/#check-your-os","title":"Check your OS","text":"<p>If you already use Linux or MacOS X, great! Ignore this paragraph!. If you use Windows, log in to your ICER HPCC account instead. The total required storage for this  workshop is ~2 GB.</p>"},{"location":"workshop_material/02_setup/#installing-miniforge","title":"Installing Miniforge","text":"<p>Information on how to install Miniforge can be found on their repository.</p>"},{"location":"workshop_material/02_setup/#what-if-i-already-have-a-conda-installation-or-i-am-on-icer","title":"What if I already have a conda installation, or I am on ICER?","text":"<p>Great! You can use your existing conda installation, whether it be Anaconda, Miniconda, Miniforge etc. On ICER, you can also load our Miniforge3 module with the command <code>module purge; module load Miniforge3</code>.</p>"},{"location":"workshop_material/02_setup/#create-a-conda-environment","title":"Create a conda environment","text":"<p>With Miniforge, we can create a conda environment which acts as a space contained from the rest of the machine in which our workflow will automatically install all the necessary software it uses, supporting the portability and reproducibility of your workflow.</p> <p>Create a conda environment (called <code>snakemake_env</code>) that has Snakemake installed (and all it's dependant software) and git (which will be used to clone this repository later)</p> <pre><code>conda create -n snakemake_env bioconda::snakemake\n</code></pre> <p>Respond yes to the following prompt to install the necessary software in the new conda environment:</p> <pre><code>Proceed ([y]/n)?\n</code></pre> <p>Note. this installed Snakemake version 6.7.0 for me, you can use the same version this workshop was created with <code>conda create -n snakemake_env snakemake=6.7.0</code></p> <p>Activate the conda environment we just created</p> <pre><code>conda activate snakemake_env\n</code></pre> <p>Now we can see which conda environment we are in on the command line, <code>(base)</code> has been replaced with <code>(snakemake_env)</code></p> <pre><code>(snakemake_env) bash-4.2$ \n</code></pre> <p>Snakemake has been installed within your <code>snakemake_env</code> environment, so you won't be able to see or use your Snakemake install unless you are within this environment</p>"},{"location":"workshop_material/02_setup/#clone-this-repo","title":"Clone this repo","text":"<p>Clone this repo with the following:</p> <p>code</p> <p></p><pre><code>git clone https://github.com/msu-cmse-courses/CMSE_890-602_snakemake.git\n</code></pre> <pre><code>cd CMSE_890-602_snakemake\n</code></pre><p></p> <p>See the Git Guides for information on cloning a repo</p> <p>Back to homepage</p>"},{"location":"workshop_material/03_create_a_basic_workflow/","title":"03 - Create a basic workflow","text":""},{"location":"workshop_material/03_create_a_basic_workflow/#03-create-a-basic-workflow","title":"03 - Create a basic workflow","text":""},{"location":"workshop_material/03_create_a_basic_workflow/#301-aim","title":"3.01 Aim","text":"<p>Let's create a basic workflow that will do some of the analysis steps for genetic data. We will have three samples with two files each - six files in total. These files will be processed through the below workflow, passing through three software.</p>  ![rulegraph_1](./images/rulegraph_1.png)  <p>We have paired end sequencing data for three samples <code>NA24631</code> to process in the <code>./data</code> directory. Let's have a look:</p> <p>code</p> <pre><code>ls -lh ./data/\n</code></pre> output <pre><code>total 13M\n-rw-rw----+ 1 lkemp nesi99991 2.1M May 11 12:06 NA24631_1.fastq.gz\n-rw-rw----+ 1 lkemp nesi99991 2.3M May 11 12:06 NA24631_2.fastq.gz\n-rw-rw----+ 1 lkemp nesi99991 2.1M May 11 12:06 NA24694_1.fastq.gz\n-rw-rw----+ 1 lkemp nesi99991 2.3M May 11 12:06 NA24694_2.fastq.gz\n-rw-rw----+ 1 lkemp nesi99991 1.8M May 11 12:06 NA24695_1.fastq.gz\n-rw-rw----+ 1 lkemp nesi99991 1.9M May 11 12:06 NA24695_2.fastq.gz\n</code></pre> <p></p>"},{"location":"workshop_material/03_create_a_basic_workflow/#302-snakemake-workflow-file-structure","title":"3.02 Snakemake workflow file structure","text":"<p>Workflow file structure:</p> <pre><code>demo_workflow/\n      |_______results/\n      |_______workflow/\n                 |_______Snakefile\n</code></pre> <p>We will create and run our workflow from the <code>workflow</code> directory send all of our file outputs/results to the <code>results</code> directory</p> <p>Read up on the best practice workflow structure here</p> <p>Create this file structure and our main Snakefile with:</p> <p>code</p> <p></p><pre><code>mkdir -p demo_workflow/{results,workflow}\n</code></pre> <pre><code>touch demo_workflow/workflow/Snakefile\n</code></pre><p></p> <p>Now you should have the very beginnings of your Snakemake workflow in a <code>demo_workflow</code> directory. Let's have a look:</p> <p>code</p> <pre><code>ls -lh demo_workflow/\n</code></pre> output <pre><code>total 1.0K\ndrwxrws---+ 2 lkemp nesi99991 4.0K May 11 12:07 results\ndrwxrws---+ 2 lkemp nesi99991 4.0K May 11 12:07 workflow\n</code></pre> <p></p> <p>code</p> <pre><code>ls -lh demo_workflow/workflow/\n</code></pre> output <pre><code>total 0\n-rw-rw----+ 1 lkemp nesi99991 0 May 11 12:07 Snakefile\n</code></pre> <p></p> <p>Within the <code>workflow</code> directory (where we will create and run our workflow), we have a <code>Snakefile</code> file that will be the backbone of our workflow.</p>"},{"location":"workshop_material/03_create_a_basic_workflow/#303-run-the-software-on-the-command-line","title":"3.03 Run the software on the command line","text":"<p>First lets run the first step in our workflow (fastqc) directly on the command line to get the syntax of the command right and check what outputs files we expect to get. Knowing what files the software will output is important for Snakemake since it is a lazy \"pull\" based system where software/rules will only run if you tell it to create the output file. We will talk more about this later!</p> <p>code</p> <ul> <li>First make sure to have fastqc available. Install it into the snakemake_env for now <pre><code>conda activate snakemake_env\n</code></pre></li> </ul> <pre><code>conda install bioconda::fastqc=0.11.9\n</code></pre> <ul> <li> <p>See what parameters are available so we know how we want to run this software before we put it in a Snakemake workflow </p><pre><code>fastqc --help\n</code></pre><p></p> </li> <li> <p>Create a test directory to put the output files </p><pre><code>mkdir fastqc_test\n</code></pre><p></p> </li> </ul> <p>Run fastqc directly on the command line on one of the samples </p><pre><code>fastqc ./data/NA24631_1.fastq.gz ./data/NA24631_2.fastq.gz -o ./fastqc_test -t 2\n</code></pre><p></p> output <pre><code>Started analysis of NA24631_1.fastq.gz\nApprox 5% complete for NA24631_1.fastq.gz\nApprox 10% complete for NA24631_1.fastq.gz\nApprox 15% complete for NA24631_1.fastq.gz\nApprox 20% complete for NA24631_1.fastq.gz\nApprox 25% complete for NA24631_1.fastq.gz\nApprox 30% complete for NA24631_1.fastq.gz\nApprox 35% complete for NA24631_1.fastq.gz\nApprox 40% complete for NA24631_1.fastq.gz\nApprox 45% complete for NA24631_1.fastq.gz\nApprox 50% complete for NA24631_1.fastq.gz\nApprox 55% complete for NA24631_1.fastq.gz\nApprox 60% complete for NA24631_1.fastq.gz\nStarted analysis of NA24631_2.fastq.gz\nApprox 65% complete for NA24631_1.fastq.gz\nApprox 5% complete for NA24631_2.fastq.gz\nApprox 70% complete for NA24631_1.fastq.gz\nApprox 10% complete for NA24631_2.fastq.gz\nApprox 75% complete for NA24631_1.fastq.gz\nApprox 15% complete for NA24631_2.fastq.gz\nApprox 80% complete for NA24631_1.fastq.gz\nApprox 20% complete for NA24631_2.fastq.gz\nApprox 25% complete for NA24631_2.fastq.gz\nApprox 85% complete for NA24631_1.fastq.gz\nApprox 90% complete for NA24631_1.fastq.gz\nApprox 30% complete for NA24631_2.fastq.gz\nApprox 35% complete for NA24631_2.fastq.gz\nApprox 95% complete for NA24631_1.fastq.gz\nApprox 40% complete for NA24631_2.fastq.gz\nAnalysis complete for NA24631_1.fastq.gz\nApprox 45% complete for NA24631_2.fastq.gz\nApprox 50% complete for NA24631_2.fastq.gz\nApprox 55% complete for NA24631_2.fastq.gz\nApprox 60% complete for NA24631_2.fastq.gz\nApprox 65% complete for NA24631_2.fastq.gz\nApprox 70% complete for NA24631_2.fastq.gz\nApprox 75% complete for NA24631_2.fastq.gz\nApprox 80% complete for NA24631_2.fastq.gz\nApprox 85% complete for NA24631_2.fastq.gz\nApprox 90% complete for NA24631_2.fastq.gz\nApprox 95% complete for NA24631_2.fastq.gz\nAnalysis complete for NA24631_2.fastq.gz\n</code></pre> <p></p> <ul> <li>What are the output files of fastqc? Find out with: <pre><code>ls -lh ./fastqc_test\n</code></pre></li> </ul> output <pre><code>total 2.5M\n-rw-rw----+ 1 lkemp nesi99991 718K May 11 12:08 NA24631_1_fastqc.html\n-rw-rw----+ 1 lkemp nesi99991 475K May 11 12:08 NA24631_1_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 726K May 11 12:08 NA24631_2_fastqc.html\n-rw-rw----+ 1 lkemp nesi99991 479K May 11 12:08 NA24631_2_fastqc.zip\n</code></pre> <p></p>"},{"location":"workshop_material/03_create_a_basic_workflow/#304-create-the-first-rule-in-your-workflow","title":"3.04 Create the first rule in your workflow","text":"<p>Let's wrap this up in a Snakemake workflow! Start with the basic structure of a Snakefile:</p> <pre><code># target OUTPUT files for the whole workflow\nrule all:\n    input:\n\n# workflow\nrule my_rule:\n    input:\n        \"\"\n    output:\n        \"\"\n    threads:\n    shell:\n        \"\"\n</code></pre> <p>Now add our fastqc rule, let's:</p> <ul> <li>Name the rule</li> <li>Fill in the the input fastq files from the <code>data</code> directory (path relative to the Snakefile)</li> <li>Fill in the output files (now you can see it's useful to know what files fastqc outputs!)</li> <li>Set the number of threads</li> <li>Write the fastqc shell command in the <code>shell:</code> section and pass the input/output variables to the shell command</li> <li>Set the final output files for the whole workflow in <code>rule all:</code></li> </ul> <p>The use of the word <code>input</code> in <code>rule all</code> can be confusing, but in this context, it is referring to the final output files of the whole workflow</p> Edit snakefile <pre><code># target OUTPUT files for the whole workflow\nrule all:\n    input:\n+       \"../results/fastqc/NA24631_1_fastqc.html\",\n+       \"../results/fastqc/NA24631_2_fastqc.html\",\n+       \"../results/fastqc/NA24631_1_fastqc.zip\",\n+       \"../results/fastqc/NA24631_2_fastqc.zip\"\n\n# workflow\n- rule my_rule:\n+ rule fastqc:\n    input:\n+       R1 = \"../../data/NA24631_1.fastq.gz\",\n+       R2 = \"../../data/NA24631_2.fastq.gz\"\n    output:\n+       html = [\"../results/fastqc/NA24631_1_fastqc.html\", \"../results/fastqc/NA24631_2_fastqc.html\"],\n+       zip = [\"../results/fastqc/NA24631_1_fastqc.zip\", \"../results/fastqc/NA24631_2_fastqc.zip\"]\n+   threads: 2\n    shell:\n+       \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads}\"\n</code></pre> Current snakefile: <pre><code># target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/fastqc/NA24631_1_fastqc.html\",\n        \"../results/fastqc/NA24631_2_fastqc.html\",\n        \"../results/fastqc/NA24631_1_fastqc.zip\",\n        \"../results/fastqc/NA24631_2_fastqc.zip\"\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/NA24631_1.fastq.gz\",\n        R2 = \"../../data/NA24631_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/NA24631_1_fastqc.html\", \"../results/fastqc/NA24631_2_fastqc.html\"],\n        zip = [\"../results/fastqc/NA24631_1_fastqc.zip\", \"../results/fastqc/NA24631_2_fastqc.zip\"]\n    threads: 2\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads}\"\n</code></pre> <p></p> <p>When you have multiple input and output files:</p> <ul> <li>You can \"name\" you inputs/outputs, they can be called separately in the shell command</li> <li>Remember to use commas between multiple inputs/outputs, it's a common source of error!</li> </ul> <p>Let's test the workflow! First we need to be in the <code>workflow</code> directory, where the Snakefile is</p> <p>code</p> <pre><code>cd demo_workflow/workflow/\n</code></pre>"},{"location":"workshop_material/03_create_a_basic_workflow/#305-dryrun","title":"3.05 Dryrun","text":"<p>Then let's carry out a dryrun of the workflow, where no actual analysis is undertaken (fastqc is not run) but the overall Snakemake structure is run/validated. This is a good way to check for errors in your Snakemake workflow before actually running your workflow.</p> <p>code</p> <p>Make sure you activate your snakemake environment</p> <pre><code>conda activate snakemake_env\n</code></pre> <pre><code>snakemake --dryrun\n</code></pre> output <pre><code>Building DAG of jobs...\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        1              1              1\ntotal         2              1              1\n\n\n[Wed May 11 12:09:56 2022]\nrule fastqc:\n    input: ../../data/NA24631_1.fastq.gz, ../../data/NA24631_2.fastq.gz\n    output: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 1\n    resources: tmpdir=/dev/shm/jobs/26763281\n\n\n[Wed May 11 12:09:56 2022]\nlocalrule all:\n    input: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 0\n    resources: tmpdir=/dev/shm/jobs/26763281\n\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        1              1              1\ntotal         2              1              1\n\nThis was a dry-run (flag -n). The order of jobs does not reflect the order of execution.\n</code></pre> <p></p> <p>The last table in the output confirms that the workflow will run one sample (<code>count 1</code>) through fastqc (<code>job fastqc</code>)</p>"},{"location":"workshop_material/03_create_a_basic_workflow/#306-create-a-dag","title":"3.06 Create a DAG","text":"<p>We can also visualise our workflow by creating a directed acyclic graph (DAG). We can tell snakemake to create a DAG with the <code>--dag</code> flag, then pipe this output to the dot software and write the output to the file, <code>dag_1.png</code></p> <p>Note</p> <p>If you don't have dot installed, you can add it to your snakemake environment with the command <code>conda install graphviz</code>.</p> <p>code</p> <pre><code>snakemake --dag | dot -Tpng &gt; dag_1.png\n</code></pre> DAG: <p></p><p></p> <p></p> <p>Our diagram has a node for each job which are connected by edges representing dependencies</p> <p>Note. this diagram can be output to several other image formats such as svg or pdf</p>"},{"location":"workshop_material/03_create_a_basic_workflow/#307-fullrun","title":"3.07 Fullrun","text":"<p>Let's do a full run of our workflow (by removing the <code>--dryrun</code> flag). We will also now need to specify the maximum number of cores to use at one time with the <code>--cores</code> flag before snakemake will run</p> <p>code</p> <pre><code>snakemake --cores 2\n</code></pre> output <pre><code>Building DAG of jobs...\nUsing shell: /usr/bin/bash\nProvided cores: 2\nRules claiming more threads will be scaled down.\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        1              2              2\ntotal         2              1              2\n\nSelect jobs to execute...\n\n[Wed May 11 12:10:44 2022]\nrule fastqc:\n    input: ../../data/NA24631_1.fastq.gz, ../../data/NA24631_2.fastq.gz\n    output: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 1\n    threads: 2\n    resources: tmpdir=/dev/shm/jobs/26763281\n\nStarted analysis of NA24631_1.fastq.gz\nApprox 5% complete for NA24631_1.fastq.gz\nApprox 10% complete for NA24631_1.fastq.gz\nApprox 15% complete for NA24631_1.fastq.gz\nApprox 20% complete for NA24631_1.fastq.gz\nApprox 25% complete for NA24631_1.fastq.gz\nApprox 30% complete for NA24631_1.fastq.gz\nApprox 35% complete for NA24631_1.fastq.gz\nApprox 40% complete for NA24631_1.fastq.gz\nApprox 45% complete for NA24631_1.fastq.gz\nApprox 50% complete for NA24631_1.fastq.gz\nApprox 55% complete for NA24631_1.fastq.gz\nApprox 60% complete for NA24631_1.fastq.gz\nApprox 65% complete for NA24631_1.fastq.gz\nStarted analysis of NA24631_2.fastq.gz\nApprox 70% complete for NA24631_1.fastq.gz\nApprox 5% complete for NA24631_2.fastq.gz\nApprox 75% complete for NA24631_1.fastq.gz\nApprox 10% complete for NA24631_2.fastq.gz\nApprox 80% complete for NA24631_1.fastq.gz\nApprox 15% complete for NA24631_2.fastq.gz\nApprox 85% complete for NA24631_1.fastq.gz\nApprox 20% complete for NA24631_2.fastq.gz\nApprox 90% complete for NA24631_1.fastq.gz\nApprox 25% complete for NA24631_2.fastq.gz\nApprox 95% complete for NA24631_1.fastq.gz\nApprox 30% complete for NA24631_2.fastq.gz\nAnalysis complete for NA24631_1.fastq.gz\nApprox 35% complete for NA24631_2.fastq.gz\nApprox 40% complete for NA24631_2.fastq.gz\nApprox 45% complete for NA24631_2.fastq.gz\nApprox 50% complete for NA24631_2.fastq.gz\nApprox 55% complete for NA24631_2.fastq.gz\nApprox 60% complete for NA24631_2.fastq.gz\nApprox 65% complete for NA24631_2.fastq.gz\nApprox 70% complete for NA24631_2.fastq.gz\nApprox 75% complete for NA24631_2.fastq.gz\nApprox 80% complete for NA24631_2.fastq.gz\nApprox 85% complete for NA24631_2.fastq.gz\nApprox 90% complete for NA24631_2.fastq.gz\nApprox 95% complete for NA24631_2.fastq.gz\nAnalysis complete for NA24631_2.fastq.gz\n[Wed May 11 12:10:48 2022]\nFinished job 1.\n1 of 2 steps (50%) done\nSelect jobs to execute...\n\n[Wed May 11 12:10:48 2022]\nlocalrule all:\n    input: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 0\n    resources: tmpdir=/dev/shm/jobs/26763281\n\n[Wed May 11 12:10:48 2022]\nFinished job 0.\n2 of 2 steps (100%) done\nComplete log: .snakemake/log/2022-05-11T121044.745212.snakemake.log\n</code></pre> <p></p> <p>It worked! Now in our results directory we have our output files from fastqc. Let's have a look:</p> <p>code</p> <pre><code>ls -lh ../results/fastqc/\n</code></pre> output <pre><code>total 2.5M\n-rw-rw----+ 1 lkemp nesi99991 718K May 11 12:10 NA24631_1_fastqc.html\n-rw-rw----+ 1 lkemp nesi99991 475K May 11 12:10 NA24631_1_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 726K May 11 12:10 NA24631_2_fastqc.html\n-rw-rw----+ 1 lkemp nesi99991 479K May 11 12:10 NA24631_2_fastqc.zip\n</code></pre> <p>{% include exercise.html title=\"e3dot10\" content=e3dot10%} </p>"},{"location":"workshop_material/03_create_a_basic_workflow/#308-lazy-evaluation","title":"3.08 Lazy evaluation","text":"<p>What happens if we try a dryrun or full run now?</p> <p>code</p> <pre><code>snakemake --dryrun --cores 2\n</code></pre> <p>output</p> <pre><code>Building DAG of jobs...\nNothing to be done (all requested files are present and up to date).\n</code></pre> <p></p> <p>code</p> <pre><code>snakemake --cores 2\n</code></pre> output <pre><code>Building DAG of jobs...\nNothing to be done (all requested files are present and up to date).\nComplete log: .snakemake/log/2022-05-11T121300.251492.snakemake.log\n</code></pre> <p></p> <p>Nothing happens, all the target files in <code>rule all</code> have already been created so Snakemake does nothing</p> <p>Also, what happens if we create another directed acyclic graph (DAG) after the workflow has been run?</p> <p>code</p> <pre><code>snakemake --dag | dot -Tpng &gt; dag_2.png\n</code></pre> DAG <p></p><p></p> <p></p> <p>Notice our workflow 'job nodes' are now dashed lines, this indicates that their output is up to date and therefore the rule doesn't need to be run. We already have our target files!</p> <p>This can be quite informative if your workflow errors out at a rule. You can visually check which rules successfully ran and which didn't.</p>"},{"location":"workshop_material/03_create_a_basic_workflow/#309-run-using-conda-environments","title":"3.09 Run using conda environments","text":"<p>fastqc worked because we loaded it in our current shell session. Let's specify the conda environment for fastqc so the user of the workflow doesn't need to load it manually.</p> <p>First, we need to specify a conda environment for fastqc.</p> <p>Make a conda environment file for fastqc</p> <pre><code># create a folder for conda environments\nmkdir envs\n\n# create the file\ntouch ./envs/fastqc.yaml\n\n# see what versions of fastqc are available in the bioconda channel\nconda search fastqc -c bioconda\n\n# write the following to fastqc.yaml\nchannels:\n  - bioconda\n  - conda-forge\n  - defaults\ndependencies:\n  - bioconda::fastqc=0.11.9\n</code></pre> <p>This will install fastqc (version 0.11.9) from bioconda into a 'clean' conda environment separate from the rest of your computer</p> <p>See here for information on creating conda environment files.</p> <p>Update our rule to use it using the <code>conda:</code> directive, pointing the rule to the <code>envs</code> directory which has our conda environment file for fastqc (directory relative to the Snakefile)</p> <pre><code># target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/fastqc/NA24631_1_fastqc.html\",\n        \"../results/fastqc/NA24631_2_fastqc.html\",\n        \"../results/fastqc/NA24631_1_fastqc.zip\",\n        \"../results/fastqc/NA24631_2_fastqc.zip\"\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/NA24631_1.fastq.gz\",\n        R2 = \"../../data/NA24631_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/NA24631_1_fastqc.html\", \"../results/fastqc/NA24631_2_fastqc.html\"],\n        zip = [\"../results/fastqc/NA24631_1_fastqc.zip\", \"../results/fastqc/NA24631_2_fastqc.zip\"]\n    threads: 2\n+   conda:\n+       \"envs/fastqc.yaml\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads}\"\n</code></pre> <p>Run again, now telling Snakemake to use to use Conda to automatically install our software by using the <code>--software-deployment-method conda</code> flag.</p> <pre><code># first remove output of last run\nrm -r ../results/*\n\n# Run dryrun again\n- snakemake --dryrun --cores 2\n+ snakemake --dryrun --cores 2 --software-deployment-method conda\n</code></pre> <p>My output:</p> <pre><code>Building DAG of jobs...\nConda environment envs/fastqc.yaml will be created.\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        1              2              2\ntotal         2              1              2\n\n\n[Mon Sep 13 03:06:45 2021]\nrule fastqc:\n    input: ../../data/NA24631_1.fastq.gz, ../../data/NA24631_2.fastq.gz\n    output: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 1\n    threads: 2\n    resources: tmpdir=/dev/shm/jobs/22281190\n\n\n[Mon Sep 13 03:06:45 2021]\nlocalrule all:\n    input: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 0\n    resources: tmpdir=/dev/shm/jobs/22281190\n\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        1              2              2\ntotal         2              1              2\n\nThis was a dry-run (flag -n). The order of jobs does not reflect the order of execution.\n</code></pre> <p>Notice it now says that \"Conda environment envs/fastqc.yaml will be created.\". Now the software our workflow uses will be automatically installed!</p> <p>Let's do a full run</p> <pre><code>- snakemake --cores 2\n+ snakemake --cores 2 --software-deployment-method conda\n</code></pre> <p>My output:</p> <pre><code>Building DAG of jobs...\nCreating conda environment envs/fastqc.yaml...\nDownloading and installing remote packages.\nEnvironment for envs/fastqc.yaml created (location: .snakemake/conda/67c1376bae89b8de73037e703ea4b6f5)\nUsing shell: /usr/bin/bash\nProvided cores: 2\nRules claiming more threads will be scaled down.\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        1              2              2\ntotal         2              1              2\n\nSelect jobs to execute...\n\n[Mon Sep 13 03:10:27 2021]\nrule fastqc:\n    input: ../../data/NA24631_1.fastq.gz, ../../data/NA24631_2.fastq.gz\n    output: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 1\n    threads: 2\n    resources: tmpdir=/dev/shm/jobs/22281190\n\nActivating conda environment: /scale_wlg_persistent/filesets/project/nesi99991/snakemake20210914/lkemp/snakemake_workshop/demo_workflow/workflow/.snakemake/conda/67c1376bae89b8de73037e703ea4b6f5\nStarted analysis of NA24631_1.fastq.gz\nApprox 5% complete for NA24631_1.fastq.gz\nApprox 10% complete for NA24631_1.fastq.gz\nApprox 15% complete for NA24631_1.fastq.gz\nApprox 20% complete for NA24631_1.fastq.gz\nApprox 25% complete for NA24631_1.fastq.gz\nApprox 30% complete for NA24631_1.fastq.gz\nApprox 35% complete for NA24631_1.fastq.gz\nApprox 40% complete for NA24631_1.fastq.gz\nApprox 45% complete for NA24631_1.fastq.gz\nApprox 50% complete for NA24631_1.fastq.gz\nApprox 55% complete for NA24631_1.fastq.gz\nApprox 60% complete for NA24631_1.fastq.gz\nStarted analysis of NA24631_2.fastq.gz\nApprox 65% complete for NA24631_1.fastq.gz\nApprox 5% complete for NA24631_2.fastq.gz\nApprox 70% complete for NA24631_1.fastq.gz\nApprox 10% complete for NA24631_2.fastq.gz\nApprox 75% complete for NA24631_1.fastq.gz\nApprox 15% complete for NA24631_2.fastq.gz\nApprox 80% complete for NA24631_1.fastq.gz\nApprox 20% complete for NA24631_2.fastq.gz\nApprox 85% complete for NA24631_1.fastq.gz\nApprox 25% complete for NA24631_2.fastq.gz\nApprox 90% complete for NA24631_1.fastq.gz\nApprox 30% complete for NA24631_2.fastq.gz\nApprox 95% complete for NA24631_1.fastq.gz\nApprox 35% complete for NA24631_2.fastq.gz\nAnalysis complete for NA24631_1.fastq.gz\nApprox 40% complete for NA24631_2.fastq.gz\nApprox 45% complete for NA24631_2.fastq.gz\nApprox 50% complete for NA24631_2.fastq.gz\nApprox 55% complete for NA24631_2.fastq.gz\nApprox 60% complete for NA24631_2.fastq.gz\nApprox 65% complete for NA24631_2.fastq.gz\nApprox 70% complete for NA24631_2.fastq.gz\nApprox 75% complete for NA24631_2.fastq.gz\nApprox 80% complete for NA24631_2.fastq.gz\nApprox 85% complete for NA24631_2.fastq.gz\nApprox 90% complete for NA24631_2.fastq.gz\nApprox 95% complete for NA24631_2.fastq.gz\nAnalysis complete for NA24631_2.fastq.gz\n[Mon Sep 13 03:10:33 2021]\nFinished job 1.\n1 of 2 steps (50%) done\nSelect jobs to execute...\n\n[Mon Sep 13 03:10:33 2021]\nlocalrule all:\n    input: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 0\n    resources: tmpdir=/dev/shm/jobs/22281190\n\n[Mon Sep 13 03:10:33 2021]\nFinished job 0.\n2 of 2 steps (100%) done\nComplete log: /scale_wlg_persistent/filesets/project/nesi99991/snakemake20210914/lkemp/snakemake_workshop/demo_workflow/workflow/.snakemake/log/2021-09-13T030734.543325.snakemake.log\n</code></pre>"},{"location":"workshop_material/03_create_a_basic_workflow/#additional-information","title":"Additional information","text":"<p>Have a look at bioconda's list of packages to see the VERY extensive list of quality open source (free) bioinformatics software that is available for download and use. Note that is only one of the conda package repositories that exist, also have a look at the conda-forge and main conda package repositories.</p> <p>Another option to run your code in self-contained and reproducible environments are containers. Snakemake can use Singularity containers to execute the workflow, as detailed in the official documentation.</p>"},{"location":"workshop_material/03_create_a_basic_workflow/#310-capture-our-logs","title":"3.10 Capture our logs","text":"<p>So far our logs (for fastqc) have been simply printed to our screen. As you can imagine, if you had a large automated workflow (that you might not be sitting at the computer watching run) you'll want to capture all that information. Therefore, any information the software spits out (including error messages!) will be kept and can be looked at once you return to your machine from your coffee break.</p> <p>We can get the logs for each rule to be written to a log file via the <code>log:</code> directive:</p> <ul> <li>It's a good idea to organise the logs by:</li> <li>Putting the logs in a directory labelled after the rule/software that was run</li> <li> <p>Labelling the log files with the sample name the software was run on</p> </li> <li> <p>Also make sure you tell the software (fastqc) to write the standard output and standard error to this log file we defined in the <code>log:</code> directive in the shell script (eg. <code>&amp;&gt; {log}</code>)</p> </li> </ul> Edit snakefile <pre><code># target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/fastqc/NA24631_1_fastqc.html\",\n        \"../results/fastqc/NA24631_2_fastqc.html\",\n        \"../results/fastqc/NA24631_1_fastqc.zip\",\n        \"../results/fastqc/NA24631_2_fastqc.zip\"\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/NA24631_1.fastq.gz\",\n        R2 = \"../../data/NA24631_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/NA24631_1_fastqc.html\", \"../results/fastqc/NA24631_2_fastqc.html\"],\n        zip = [\"../results/fastqc/NA24631_1_fastqc.zip\", \"../results/fastqc/NA24631_2_fastqc.zip\"]\n+   log:\n+       \"logs/fastqc/NA24631.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n-       \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads}\"\n+       \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n</code></pre> Current snakefile <pre><code># target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/fastqc/NA24631_1_fastqc.html\",\n        \"../results/fastqc/NA24631_2_fastqc.html\",\n        \"../results/fastqc/NA24631_1_fastqc.zip\",\n        \"../results/fastqc/NA24631_2_fastqc.zip\"\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/NA24631_1.fastq.gz\",\n        R2 = \"../../data/NA24631_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/NA24631_1_fastqc.html\", \"../results/fastqc/NA24631_2_fastqc.html\"],\n        zip = [\"../results/fastqc/NA24631_1_fastqc.zip\", \"../results/fastqc/NA24631_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/NA24631.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n</code></pre> <p></p> <p>A tangent about standard streams</p> <ul> <li>These are standard streams in which information is returned by a computer process - in our case the logs that we see returned to us on our screen when we run fastqc</li> <li>There are two main streams:</li> <li>standard output (the log messages)</li> <li>standard error (the error messages)</li> </ul> <p>Different ways to write log files:</p> Syntax standard output in terminal standard error in terminal standard output in file standard error in file <code>&gt;</code> NO YES YES NO <code>2&gt;</code> YES NO NO YES <code>&amp;&gt;</code> NO NO YES YES <p>(Table adapted from here)</p> <p>Run again</p> <p></p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun/run again\nsnakemake --dryrun --cores 2 --software-deployment-method conda\nsnakemake --cores 2 --software-deployment-method conda\n</code></pre><p></p> output <pre><code>Building DAG of jobs...\nUsing shell: /usr/bin/bash\nProvided cores: 2\nRules claiming more threads will be scaled down.\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        1              2              2\ntotal         2              1              2\n\nSelect jobs to execute...\n\n[Wed May 11 12:15:16 2022]\nrule fastqc:\n    input: ../../data/NA24631_1.fastq.gz, ../../data/NA24631_2.fastq.gz\n    output: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    log: logs/fastqc/NA24631.log\n    jobid: 1\n    threads: 2\n    resources: tmpdir=/dev/shm/jobs/26763281\n\nActivating environment modules: FastQC/0.11.9\n\nThe following modules were not unloaded:\n   (Use \"module --force purge\" to unload all):\n\n  1) XALT/minimal   2) slurm   3) NeSI\n[Wed May 11 12:15:20 2022]\nFinished job 1.\n1 of 2 steps (50%) done\nSelect jobs to execute...\n\n[Wed May 11 12:15:20 2022]\nlocalrule all:\n    input: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    jobid: 0\n    resources: tmpdir=/dev/shm/jobs/26763281\n\n[Wed May 11 12:15:20 2022]\nFinished job 0.\n2 of 2 steps (100%) done\nComplete log: .snakemake/log/2022-05-11T121516.368334.snakemake.log\n</code></pre> <p></p> <p>We now have a log file, lets have a look at the first 10 lines of our log with:</p> <pre><code>head ./logs/fastqc/NA24631.log\n</code></pre> output <pre><code>Started analysis of NA24631_1.fastq.gz\nApprox 5% complete for NA24631_1.fastq.gz\nApprox 10% complete for NA24631_1.fastq.gz\nApprox 15% complete for NA24631_1.fastq.gz\nApprox 20% complete for NA24631_1.fastq.gz\nApprox 25% complete for NA24631_1.fastq.gz\nApprox 30% complete for NA24631_1.fastq.gz\nApprox 35% complete for NA24631_1.fastq.gz\nApprox 40% complete for NA24631_1.fastq.gz\nApprox 45% complete for NA24631_1.fastq.gz\n</code></pre> <p></p> <p>We have logs. Tidy logs.</p> ![logs](https://miro.medium.com/max/2560/1*ohWUB5snJRaMe-vJ8HaoiA.png){width=\"400\"} <p>Exercise:</p> <p>Try creating an error in the shell command (for example remove the <code>-o</code> flag) and use the three different syntaxes for writing to your log file. What is and isn't printed to your screen and to your log file?</p>"},{"location":"workshop_material/03_create_a_basic_workflow/#311-scale-up-to-analyse-all-of-our-samples","title":"3.11 Scale up to analyse all of our samples","text":"<p>We are currently only analysing one of our three samples</p> <p>Let's scale up to run all of our samples by using wildcards, this way we can grab all the samples/files in the <code>data</code> directory and analyse them</p> <ul> <li>Set a global wildcard that defines the samples to be analysed</li> <li>Generalise where this rule uses an individual sample (<code>NA24631</code>) to use this wildcard <code>{sample}</code></li> <li>Use the expand function (<code>expand()</code>) function to tell snakemake that <code>{sample}</code> is what we defined in our global wildcard <code>SAMPLES,</code></li> <li>Snakemake can figure out what <code>{sample}</code> is in our rule since it's defined in the targets in <code>rule all:</code></li> </ul> Edit snakefile <pre><code># define samples from data directory using wildcards\n+ SAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n-       \"../results/fastqc/NA24631_1_fastqc.html\",\n-       \"../results/fastqc/NA24631_2_fastqc.html\",\n-       \"../results/fastqc/NA24631_1_fastqc.zip\",\n-       \"../results/fastqc/NA24631_2_fastqc.zip\"\n+       expand(\"../results/fastqc/{sample}_1_fastqc.html\", sample = SAMPLES),\n+       expand(\"../results/fastqc/{sample}_2_fastqc.html\", sample = SAMPLES),\n+       expand(\"../results/fastqc/{sample}_1_fastqc.zip\", sample = SAMPLES),\n+       expand(\"../results/fastqc/{sample}_2_fastqc.zip\", sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n-       R1 = \"../../data/NA24631_1.fastq.gz\",\n-       R2 = \"../../data/NA24631_2.fastq.gz\"\n+       R1 = \"../../data/{sample}_1.fastq.gz\",\n+       R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n-       html = [\"../results/fastqc/NA24631_1_fastqc.html\", \"../results/fastqc/NA24631_2_fastqc.html\"],\n-       zip = [\"../results/fastqc/NA24631_1_fastqc.zip\", \"../results/fastqc/NA24631_2_fastqc.zip\"]\n+       html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n+       zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n-       \"logs/fastqc/NA24631.log\"\n+       \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n</code></pre> Current snakefile: <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        expand(\"../results/fastqc/{sample}_1_fastqc.html\", sample = SAMPLES),\n        expand(\"../results/fastqc/{sample}_2_fastqc.html\", sample = SAMPLES),\n        expand(\"../results/fastqc/{sample}_1_fastqc.zip\", sample = SAMPLES),\n        expand(\"../results/fastqc/{sample}_2_fastqc.zip\", sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n</code></pre> <p></p> <p>Visualise workflow</p> <pre><code>snakemake --dag | dot -Tpng &gt; dag_3.png\n</code></pre> <ul> <li>Now we have three samples running though our workflow, one of which has already been run in our last run (NA24631) indicated by the dashed lines</li> </ul> DAG <p></p><p></p> <p></p> <p>Run workflow again</p> <p></p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun again\nsnakemake --dryrun --cores 2 --software-deployment-method conda\n</code></pre><p></p> <ul> <li>See how it now runs over all three of our samples in the output of the dryrun</li> </ul> output <pre><code>Building DAG of jobs...\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        3              2              2\ntotal         4              1              2\n\n\n[Wed May 11 12:16:46 2022]\nrule fastqc:\n    input: ../../data/NA24695_1.fastq.gz, ../../data/NA24695_2.fastq.gz\n    output: ../results/fastqc/NA24695_1_fastqc.html, ../results/fastqc/NA24695_2_fastqc.html, ../results/fastqc/NA24695_1_fastqc.zip, ../results/fastqc/NA24695_2_fastqc.zip\n    log: logs/fastqc/NA24695.log\n    jobid: 2\n    wildcards: sample=NA24695\n    threads: 2\n    resources: tmpdir=/dev/shm/jobs/26763281\n\n\n[Wed May 11 12:16:46 2022]\nrule fastqc:\n    input: ../../data/NA24631_1.fastq.gz, ../../data/NA24631_2.fastq.gz\n    output: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    log: logs/fastqc/NA24631.log\n    jobid: 1\n    wildcards: sample=NA24631\n    threads: 2\n    resources: tmpdir=/dev/shm/jobs/26763281\n\n\n[Wed May 11 12:16:46 2022]\nrule fastqc:\n    input: ../../data/NA24694_1.fastq.gz, ../../data/NA24694_2.fastq.gz\n    output: ../results/fastqc/NA24694_1_fastqc.html, ../results/fastqc/NA24694_2_fastqc.html, ../results/fastqc/NA24694_1_fastqc.zip, ../results/fastqc/NA24694_2_fastqc.zip\n    log: logs/fastqc/NA24694.log\n    jobid: 3\n    wildcards: sample=NA24694\n    threads: 2\n    resources: tmpdir=/dev/shm/jobs/26763281\n\n\n[Wed May 11 12:16:46 2022]\nlocalrule all:\n    input: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24695_1_fastqc.html, ../results/fastqc/NA24694_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24695_2_fastqc.html, ../results/fastqc/NA24694_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24695_1_fastqc.zip, ../results/fastqc/NA24694_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip, ../results/fastqc/NA24695_2_fastqc.zip, ../results/fastqc/NA24694_2_fastqc.zip\n    jobid: 0\n    resources: tmpdir=/dev/shm/jobs/26763281\n\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        3              2              2\ntotal         4              1              2\n\nThis was a dry-run (flag -n). The order of jobs does not reflect the order of execution.\n</code></pre> <p></p> <p>code</p> <pre><code># full run again\nsnakemake --cores 2 --software-deployment-method conda\n</code></pre> <ul> <li>All three samples were run through our workflow! And we have a log file for each sample for the fastqc rule</li> </ul> <pre><code>ls -lh ./logs/fastqc\n</code></pre> output <pre><code>total 1.5K\n-rw-rw----+ 1 lkemp nesi99991 1.8K May 11 12:17 NA24631.log\n-rw-rw----+ 1 lkemp nesi99991 1.8K May 11 12:17 NA24694.log\n-rw-rw----+ 1 lkemp nesi99991 1.8K May 11 12:17 NA24695.log\n</code></pre> <p></p>"},{"location":"workshop_material/03_create_a_basic_workflow/#312-add-more-rules","title":"3.12 Add more rules","text":"<p>Make a conda environment file for multiqc</p> <pre><code># create the file\ntouch ./envs/multiqc.yaml\n\n# see what versions of fastqc are available in the bioconda channel\nconda search multiqc -c bioconda\n\n# write the following to multiqc.yaml\nchannels:\n  - conda-forge\n  - bioconda\n  - defaults\ndependencies:\n  - bioconda::multiqc=1.17\n</code></pre> <ul> <li>Connect the outputs of fastqc to the inputs of multiqc</li> <li>Add a new final target for <code>rule all:</code></li> </ul> Edit snakefile <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        expand(\"../results/fastqc/{sample}_1_fastqc.html\", sample = SAMPLES),\n        expand(\"../results/fastqc/{sample}_2_fastqc.html\", sample = SAMPLES),\n        expand(\"../results/fastqc/{sample}_1_fastqc.zip\", sample = SAMPLES),\n-       expand(\"../results/fastqc/{sample}_2_fastqc.zip\", sample = SAMPLES)\n+       expand(\"../results/fastqc/{sample}_2_fastqc.zip\", sample = SAMPLES),\n+       \"../results/multiqc_report.html\"\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n\n+ rule multiqc:\n+   input:\n+       expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n+   output:\n+       \"../results/multiqc_report.html\"\n+   log:\n+       \"logs/multiqc/multiqc.log\"\n+   conda:\n+       \"envs/multiqc.yaml\"\n+   shell:\n+       \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n</code></pre> Current snakefile: <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        expand(\"../results/fastqc/{sample}_1_fastqc.html\", sample = SAMPLES),\n        expand(\"../results/fastqc/{sample}_2_fastqc.html\", sample = SAMPLES),\n        expand(\"../results/fastqc/{sample}_1_fastqc.zip\", sample = SAMPLES),\n        expand(\"../results/fastqc/{sample}_2_fastqc.zip\", sample = SAMPLES),\n        \"../results/multiqc_report.html\"\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n    shell:\n        \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n</code></pre> <p></p> <p>Run workflow again</p> <p></p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun/run again\nsnakemake --dryrun --cores 2 --software-deployment-method conda\nsnakemake --cores 2 --software-deployment-method conda\n</code></pre><p></p> <ul> <li> <p>Visualise workflow </p><pre><code>snakemake --dag | dot -Tpng &gt; dag_4.png\n</code></pre><p></p> </li> <li> <p>Now we have two rules in our workflow (fastqc and multiqc), we can also see that multiqc isn't run for each sample (since it merges the output of fastqc for all samples)</p> </li> </ul> DAG: <p></p> <p></p>"},{"location":"workshop_material/03_create_a_basic_workflow/#313-more-about-snakemakes-lazy-behaviour","title":"3.13 More about Snakemake's lazy behaviour","text":"<p>What happens if we only have the final target file (<code>../results/multiqc_report.html</code>) in <code>rule all:</code></p> Edit snakefile <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n-       expand(\"../results/fastqc/{sample}_1_fastqc.html\", sample = SAMPLES),\n-       expand(\"../results/fastqc/{sample}_2_fastqc.html\", sample = SAMPLES),\n-       expand(\"../results/fastqc/{sample}_1_fastqc.zip\", sample = SAMPLES),\n-       expand(\"../results/fastqc/{sample}_2_fastqc.zip\", sample = SAMPLES),\n        \"../results/multiqc_report.html\"\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n    shell:\n        \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n</code></pre> Current snakefile: <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\"\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n    shell:\n        \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n</code></pre> <p></p> <p>Run workflow again</p> <p></p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun again\nsnakemake --dryrun --cores 2 --software-deployment-method conda\n</code></pre><p></p> <ul> <li> <p>It still works because it is the last file in the workflow sequence, Snakemake will do all the steps necessary to get to this target file (therefore it runs both fastqc and multiqc)</p> </li> <li> <p>Visualise workflow   </p><pre><code>snakemake --dag | dot -Tpng &gt; dag_5.png\n</code></pre><p></p> </li> <li> <p>Although the workflow ran the same, the DAG actually changed slightly, now there is only one file target and only the output of multiqc goes to <code>rule all</code></p> </li> </ul> DAG <p></p> <p></p> <p>Beware: Snakemake will also NOT run rules that it doesn't need to run in order to get the target files defined in rule: all</p> <p>For example if only our fastqc outputs are defined as the target in <code>rule: all</code></p> Edit snakefile <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n+       expand(\"../results/fastqc/{sample}_1_fastqc.html\", sample = SAMPLES),\n+       expand(\"../results/fastqc/{sample}_2_fastqc.html\", sample = SAMPLES),\n+       expand(\"../results/fastqc/{sample}_1_fastqc.zip\", sample = SAMPLES),\n+       expand(\"../results/fastqc/{sample}_2_fastqc.zip\", sample = SAMPLES)\n-       \"../results/multiqc_report.html\"\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n    shell:\n        \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n</code></pre> Current snakefile: <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        expand(\"../results/fastqc/{sample}_1_fastqc.html\", sample = SAMPLES),\n        expand(\"../results/fastqc/{sample}_2_fastqc.html\", sample = SAMPLES),\n        expand(\"../results/fastqc/{sample}_1_fastqc.zip\", sample = SAMPLES),\n        expand(\"../results/fastqc/{sample}_2_fastqc.zip\", sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n    shell:\n        \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n</code></pre> <p></p> <p>Run again</p> <pre><code># run dryrun again\nsnakemake --dryrun --cores 2 --software-deployment-method conda\n</code></pre> <p>My partial output:</p> <pre><code>Job stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nall           1              1              1\nfastqc        3              2              2\ntotal         4              1              2\n</code></pre> <p></p> <p>Our multiqc rule won't be run/evaluated : Visualise workflow</p> <pre><code>snakemake --dag | dot -Tpng &gt; dag_6.png\n</code></pre> <ul> <li>Now we are back to only running fastqc in our workflow, despite having our second rule (multiqc) in our workflow</li> </ul> DAG: <p></p> <p></p> <p>Snakemake is lazy.</p> <p></p>"},{"location":"workshop_material/03_create_a_basic_workflow/#314-add-even-more-rules","title":"3.14 Add even more rules","text":"<p>Let's add the rest of the rules. We want to get to:</p> ![rulegraph_1](./images/rulegraph_1.png) <p>We currently have fastqc and multiqc, so we still need to add trim_galore</p> <pre><code># create the file\ntouch ./envs/trimgalore.yaml\n\n# see what versions of fastqc are available in the bioconda channel\nconda search trim-galore -c bioconda\n\n# write the following to fastqc.yaml\nchannels:\n  - bioconda\n  - conda-forge\n  - defaults\ndependencies:\n  - bioconda::trim-galore=0.6.10\n</code></pre> Edit snakefile <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n-       expand(\"../results/fastqc/{sample}_1_fastqc.html\", sample = SAMPLES),\n-       expand(\"../results/fastqc/{sample}_2_fastqc.html\", sample = SAMPLES),\n-       expand(\"../results/fastqc/{sample}_1_fastqc.zip\", sample = SAMPLES),\n-       expand(\"../results/fastqc/{sample}_2_fastqc.zip\", sample = SAMPLES)\n+       \"../results/multiqc_report.html\",\n+       expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n    shell:\n        \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n\n+ rule trim_galore:\n+   input:\n+       [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n+   output:\n+       [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n+   log:\n+       \"logs/trim_galore/{sample}.log\"\n+   conda:\n+    \"envs/trimgalore.yaml\"\n+   threads: 2\n+   shell:\n+       \"trim_galore {input} -o ../results/trimmed/ --paired --cores {threads} &amp;&gt; {log}\"\n</code></pre> Current snakefile: <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n    shell:\n        \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n    log:\n        \"logs/trim_galore/{sample}.log\"\n    conda:\n        \"envs/trimgalore.yaml\"\n    threads: 2\n    shell:\n        \"trim_galore {input} -o ../results/trimmed/ --paired --cores {threads} &amp;&gt; {log}\"\n</code></pre> <p></p> <p>Visualise workflow</p> <pre><code>snakemake --dag | dot -Tpng &gt; dag_7.png\n</code></pre> <p>Fantastic, we are starting to build a workflow!</p> DAG: <p></p> <p></p> <p>However, when analysing many samples, our DAG can become messy and complicated. Instead, we can create a rulegraph that will let us visualise our workflow without showing every single sample that will run through it</p> <p>code</p> <pre><code>snakemake --rulegraph | dot -Tpng &gt; rulegraph_1.png\n</code></pre> <p>My rulegraph:</p> <p></p> <p></p> <p>An aside: another option that will show all your input and output files at each step:</p> <pre><code>snakemake --filegraph | dot -Tpng &gt; filegraph.png\n</code></pre> My filegraph: <p></p> <p></p> <p>Run the rest of the workflow</p> <p>code</p> <pre><code># run dryrun/run again\nsnakemake --dryrun --cores 2 --software-deployment-method conda\nsnakemake --cores 2 --software-deployment-method conda\n</code></pre> <p>Notice it will run only one rule/sample/file at a time...why is that?</p>"},{"location":"workshop_material/03_create_a_basic_workflow/#315-throw-it-more-cores","title":"3.15 Throw it more cores","text":"<p>Run again allowing Snakemake to use more cores overall <code>--cores 4</code> rather than <code>--cores 2</code></p> <p>code</p> <p></p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun/run again\nsnakemake --dryrun --cores 4 --software-deployment-method conda\nsnakemake --cores 4 --software-deployment-method conda\n</code></pre><p></p> <p>Notice the whole workflow ran much faster and several samples/files/rules were running at one time. This is because we set each rule to run with 2 threads. Initially we specified that the maximum number of cores to be used by the workflow was 2 with the <code>--cores 2</code> flag, meaning only one rule and sample can be run at one time. When we increased the maximum number of cores to be used by the workflow to 4 with <code>--cores 4</code>, up to 2 samples could be run through at one time.</p>"},{"location":"workshop_material/03_create_a_basic_workflow/#316-throw-it-even-more-cores","title":"3.16 Throw it even more cores","text":"<p>With a high performance cluster such as ICER, you can start to REALLY scale up, particularly when you have many samples to analyse or files to process. This is because the number of cores available in a HPC is HUGE compared to a laptop or even an high end server.</p> <p>Boom! Scalability here we come!</p> <p>To run the workflow on the cluster, we need to ensure that each step is run as a dedicated job in the queuing system of the HPC. On ICER, the queuing system is managed by Slurm.</p> <p>First, we must install an executor plugin for SLURM:</p> <p>code</p> <pre><code>conda install bioconda::snakemake-executor-plugin-slurm\n</code></pre> <p>Use the <code>--executor</code> option to specify the job submission command, using <code>slurm</code> on ICER. This command defines resources used for each job (maximum time, memory, number of cores...). In addition, you need to specify a maximum number of concurrent jobs using <code>--jobs</code>.</p> <p>ICER does not have conda installed as a default module, so there are two options to proceed.</p> <ol> <li>Edit your <code>~/.bashrc</code> file to automatically swap the Python module with Miniforge in your terminal.</li> <li>Use the pre-installed environment modules for each rule with the <code>envmodules</code> directive.</li> </ol>"},{"location":"workshop_material/03_create_a_basic_workflow/#editing-bashrc","title":"Editing <code>.bashrc</code>","text":"<ol> <li>Open your <code>~/.bashrc</code> file in a text editor.</li> <li>At the end of the file, add the line</li> </ol> <pre><code>module swap Python-bundle-PyPI Miniforge3\n</code></pre>"},{"location":"workshop_material/03_create_a_basic_workflow/#using-the-envmodules-directive","title":"Using the <code>envmodules</code> directive","text":"<p>In your snakefile, modify the <code>fastqc</code> rule by adding:</p> <pre><code>envmodules:\n    \"FastQC/0.12.1-Java-11\"\n</code></pre> <p>modify the <code>multiqc</code> rule by adding:</p> <pre><code>envmodules:\n    \"MultiQC/1.14-foss-2022b\"\n</code></pre> <p>modify the <code>trim_galore</code> rule by adding:</p> <pre><code>envmodules:\n    \"TrimGalore/0.6.10-GCCcore-12.2.0\"\n</code></pre> <p>which results in the snakefile:</p> Modified snakefile: <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    envmodules:\n        \"FastQC/0.12.1-Java-11\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n    envmodules:\n        \"MultiQC/1.14-foss-2022b\"\n    shell:\n        \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n    log:\n        \"logs/trim_galore/{sample}.log\"\n    conda:\n        \"envs/trimgalore.yaml\"\n    envmodules:\n        \"TrimGalore/0.6.10-GCCcore-12.2.0\"\n    threads: 2\n    shell:\n        \"trim_galore {input} -o ../results/trimmed/ --paired --cores {threads} &amp;&gt; {log}\"\n</code></pre> <p>Note</p> <p>If you are using environment modules, change the <code>--software-deployment-method</code> argument to <code>env-modules</code>.</p> <p>code</p> <p></p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run again on the cluster\nsnakemake --executor slurm --default-resources runtime=10 mem_mb=512 cpus_per_task=8 --jobs 10 --software-deployment-method conda\n</code></pre><p></p> output <pre><code>Building DAG of jobs...\nUsing shell: /usr/bin/bash\nProvided cores: 4\nRules claiming more threads will be scaled down.\nJob stats:\njob            count    min threads    max threads\n-----------  -------  -------------  -------------\nall                1              1              1\nfastqc             3              2              2\nmultiqc            1              1              1\ntrim_galore        3              2              2\ntotal              8              1              2\n\nSelect jobs to execute...\n\n[Wed May 11 12:26:39 2022]\nrule fastqc:\n    input: ../../data/NA24694_1.fastq.gz, ../../data/NA24694_2.fastq.gz\n    output: ../results/fastqc/NA24694_1_fastqc.html, ../results/fastqc/NA24694_2_fastqc.html, ../results/fastqc/NA24694_1_fastqc.zip, ../results/fastqc/NA24694_2_fastqc.zip\n    log: logs/fastqc/NA24694.log\n    jobid: 4\n    wildcards: sample=NA24694\n    threads: 2\n    resources: tmpdir=/dev/shm/jobs/26763281\n\nActivating environment modules: FastQC/0.11.9\n\n[Wed May 11 12:26:39 2022]\nrule trim_galore:\n    input: ../../data/NA24694_1.fastq.gz, ../../data/NA24694_2.fastq.gz\n    output: ../results/trimmed/NA24694_1_val_1.fq.gz, ../results/trimmed/NA24694_2_val_2.fq.gz\n    log: logs/trim_galore/NA24694.log\n    jobid: 7\n    wildcards: sample=NA24694\n    threads: 2\n    resources: tmpdir=/dev/shm/jobs/26763281\n\nActivating environment modules: TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\n\nThe following modules were not unloaded:\n   (Use \"module --force purge\" to unload all):\n\n  1) XALT/minimal   2) slurm   3) NeSI\n\nThe following modules were not unloaded:\n   (Use \"module --force purge\" to unload all):\n\n  1) XALT/minimal   2) slurm   3) NeSI\n[Wed May 11 12:26:44 2022]\nFinished job 4.\n1 of 8 steps (12%) done\nSelect jobs to execute...\n\n[Wed May 11 12:26:44 2022]\nrule fastqc:\n    input: ../../data/NA24631_1.fastq.gz, ../../data/NA24631_2.fastq.gz\n    output: ../results/fastqc/NA24631_1_fastqc.html, ../results/fastqc/NA24631_2_fastqc.html, ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip\n    log: logs/fastqc/NA24631.log\n    jobid: 2\n    wildcards: sample=NA24631\n    threads: 2\n    resources: tmpdir=/dev/shm/jobs/26763281\n\nActivating environment modules: FastQC/0.11.9\n\nThe following modules were not unloaded:\n   (Use \"module --force purge\" to unload all):\n\n  1) XALT/minimal   2) slurm   3) NeSI\n[Wed May 11 12:26:47 2022]\nFinished job 7.\n2 of 8 steps (25%) done\nSelect jobs to execute...\n\n[Wed May 11 12:26:47 2022]\nrule trim_galore:\n    input: ../../data/NA24631_1.fastq.gz, ../../data/NA24631_2.fastq.gz\n    output: ../results/trimmed/NA24631_1_val_1.fq.gz, ../results/trimmed/NA24631_2_val_2.fq.gz\n    log: logs/trim_galore/NA24631.log\n    jobid: 5\n    wildcards: sample=NA24631\n    threads: 2\n    resources: tmpdir=/dev/shm/jobs/26763281\n\nActivating environment modules: TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\n\nThe following modules were not unloaded:\n   (Use \"module --force purge\" to unload all):\n\n  1) XALT/minimal   2) slurm   3) NeSI\n[Wed May 11 12:26:50 2022]\nFinished job 2.\n3 of 8 steps (38%) done\nSelect jobs to execute...\n\n[Wed May 11 12:26:50 2022]\nrule fastqc:\n    input: ../../data/NA24695_1.fastq.gz, ../../data/NA24695_2.fastq.gz\n    output: ../results/fastqc/NA24695_1_fastqc.html, ../results/fastqc/NA24695_2_fastqc.html, ../results/fastqc/NA24695_1_fastqc.zip, ../results/fastqc/NA24695_2_fastqc.zip\n    log: logs/fastqc/NA24695.log\n    jobid: 3\n    wildcards: sample=NA24695\n    threads: 2\n    resources: tmpdir=/dev/shm/jobs/26763281\n\nActivating environment modules: FastQC/0.11.9\n\nThe following modules were not unloaded:\n   (Use \"module --force purge\" to unload all):\n\n  1) XALT/minimal   2) slurm   3) NeSI\n[Wed May 11 12:26:54 2022]\nFinished job 3.\n4 of 8 steps (50%) done\nSelect jobs to execute...\n\n[Wed May 11 12:26:54 2022]\nrule trim_galore:\n    input: ../../data/NA24695_1.fastq.gz, ../../data/NA24695_2.fastq.gz\n    output: ../results/trimmed/NA24695_1_val_1.fq.gz, ../results/trimmed/NA24695_2_val_2.fq.gz\n    log: logs/trim_galore/NA24695.log\n    jobid: 6\n    wildcards: sample=NA24695\n    threads: 2\n    resources: tmpdir=/dev/shm/jobs/26763281\n\nActivating environment modules: TrimGalore/0.6.7-gimkl-2020a-Python-3.8.2-Perl-5.30.1\n\nThe following modules were not unloaded:\n   (Use \"module --force purge\" to unload all):\n\n  1) XALT/minimal   2) slurm   3) NeSI\n[Wed May 11 12:26:56 2022]\nFinished job 5.\n5 of 8 steps (62%) done\nSelect jobs to execute...\n\n[Wed May 11 12:26:56 2022]\nrule multiqc:\n    input: ../results/fastqc/NA24631_1_fastqc.zip, ../results/fastqc/NA24695_1_fastqc.zip, ../results/fastqc/NA24694_1_fastqc.zip, ../results/fastqc/NA24631_2_fastqc.zip, ../results/fastqc/NA24695_2_fastqc.zip, ../results/fastqc/NA24694_2_fastqc.zip\n    output: ../results/multiqc_report.html\n    log: logs/multiqc/multiqc.log\n    jobid: 1\n    resources: tmpdir=/dev/shm/jobs/26763281\n\nActivating environment modules: MultiQC/1.9-gimkl-2020a-Python-3.8.2\n\nThe following modules were not unloaded:\n   (Use \"module --force purge\" to unload all):\n\n  1) XALT/minimal   2) slurm   3) NeSI\n[Wed May 11 12:27:01 2022]\nFinished job 6.\n6 of 8 steps (75%) done\n[Wed May 11 12:27:03 2022]\nFinished job 1.\n7 of 8 steps (88%) done\nSelect jobs to execute...\n\n[Wed May 11 12:27:03 2022]\nlocalrule all:\n    input: ../results/multiqc_report.html, ../results/trimmed/NA24631_1_val_1.fq.gz, ../results/trimmed/NA24695_1_val_1.fq.gz, ../results/trimmed/NA24694_1_val_1.fq.gz, ../results/trimmed/NA24631_2_val_2.fq.gz, ../results/trimmed/NA24695_2_val_2.fq.gz, ../results/trimmed/NA24694_2_val_2.fq.gz\n    jobid: 0\n    resources: tmpdir=/dev/shm/jobs/26763281\n\n[Wed May 11 12:27:03 2022]\nFinished job 0.\n8 of 8 steps (100%) done\nComplete log: .snakemake/log/2022-05-11T122639.019945.snakemake.log\n</code></pre> <p></p> <p>If you open another terminal on the HPC, you can use the <code>squeue</code> command to list of your jobs and their state (pending, running, etc.):</p> <p>code</p> <pre><code>squeue --me\n</code></pre> output <pre><code>JOBID         USER     ACCOUNT   NAME        CPUS MIN_MEM PARTITI START_TIME     TIME_LEFT STATE    NODELIST(REASON)    \n26763281      lkemp    nesi99991 spawner-jupy   4      4G interac 2022-05-11T1     7:30:33 RUNNING  wbn003              \n26763418      lkemp    nesi99991 snakejob.fas   8    512M large   2022-05-11T1        9:59 RUNNING  wbn096              \n26763419      lkemp    nesi99991 snakejob.tri   8    512M large   2022-05-11T1        9:59 RUNNING  wbn096              \n26763420      lkemp    nesi99991 snakejob.fas   8    512M large   2022-05-11T1        9:59 RUNNING  wbn110              \n26763421      lkemp    nesi99991 snakejob.fas   8    512M large   2022-05-11T1        9:59 RUNNING  wbn069              \n26763422      lkemp    nesi99991 snakejob.tri   8    512M large   2022-05-11T1        9:59 RUNNING  wbn070              \n26763423      lkemp    nesi99991 snakejob.tri   8    512M large   2022-05-11T1        9:59 RUNNING  wbn090  \n</code></pre> <p></p> <p>An additional trick is to use the <code>watch</code> command to repeatedly call any command in the terminal, giving you a lightweight monitoring tool ;-). Here we will use it to see your jobs gets queued and executed in real time:</p> <p>code</p> <pre><code>watch squeue --me\n</code></pre> <p>You can exit the view create by <code>watch</code> by pressing CTRL+C.</p>"},{"location":"workshop_material/03_create_a_basic_workflow/#takeaways","title":"Takeaways","text":"<ul> <li>Once familiar with environment modules, the software are very straightforward to integrate in your snakemake workflow</li> <li>Run your commands directly on the command line before wrapping it up in a Snakemake rule</li> <li>First do a dryrun to check the Snakemake structure is set up correctly</li> <li>Work iteratively (get each rule working before moving onto the next)</li> <li>File paths are relative to the Snakefile</li> <li>Run your workflow from where your Snakefile is</li> <li>Visualise your workflow by creating a DAG (directed acyclic graph), a rulegraph or filegraph</li> <li>Use environment modules to load software in your workflow - this improves reproducibility</li> <li>Snakemake is lazy...</li> <li>It will only do something if it hasn't already done it</li> <li>It will pick up where it left off, rather than run the whole workflow again</li> <li>It won't do any steps that aren't necessary to get to the target files defined in <code>rule: all</code></li> <li><code>input:</code> <code>output:</code> <code>log:</code> and <code>threads:</code> directives need to be called in the <code>shell</code> directive</li> <li>Capture your log files</li> <li>Organise your log files by naming them after the rule that was run and sample that was analysed</li> <li>You don't need to specify all the target files in <code>rule all:</code>, the final file in a given chain of tasks will suffice</li> <li>We can massively speed up our analyses by running our samples in parallel</li> </ul>"},{"location":"workshop_material/03_create_a_basic_workflow/#summary-commands","title":"Summary commands","text":"<p>code</p> <ul> <li>Create a directed acyclic graph (DAG) with:</li> </ul> <pre><code>snakemake --dag | dot -Tpng &gt; dag.png\n</code></pre> <ul> <li>Create a rulegraph with:</li> </ul> <pre><code>snakemake --rulegraph | dot -Tpng &gt; rulegraph.png\n</code></pre> <ul> <li>Create a filegraph with:</li> </ul> <pre><code>snakemake --filegraph | dot -Tpng &gt; filegraph.png\n</code></pre> <ul> <li>Run a dryrun of your snakemake workflow with:</li> </ul> <pre><code>snakemake --dryrun\n</code></pre> <ul> <li>Run your snakemake workflow with:</li> </ul> <pre><code>snakemake --cores 2\n</code></pre> <ul> <li>Run a dryrun of your snakemake workflow (using environment modules to load your software) with:</li> </ul> <pre><code>snakemake --dryrun --cores 2 --software-deployment-method conda\n</code></pre> <ul> <li>Run your snakemake workflow (using environment modules to load your software) with:</li> </ul> <pre><code>snakemake --cores 2 --software-deployment-method conda\n</code></pre> <ul> <li>Run your snakemake workflow using multiple jobs on ICER:</li> </ul> <pre><code>conda install bioconda::snakemake-executor-plugin-slurm\nsnakemake --executor slurm --default-resources runtime=10 mem_mb=512 cpus_per_task=8 --jobs 10 --software-deployment-method conda\n</code></pre> <ul> <li>Create a global wildcard to get process all your samples in a directory with:</li> </ul> <pre><code>SAMPLES, = glob_wildcards(\"../relative/path/to/samples/{sample}_1.fastq.gz\")\n</code></pre> <ul> <li>Combine this with the expand function to tell Snakemake to look at your global wildcard to figure out what you refer to as <code>{sample}</code> in your workflow</li> </ul> <pre><code>expand(\"../results/{sample}_1.fastq.gz\", sample = SAMPLES)\n</code></pre> <ul> <li>Increase the number of samples that can be analysed at one time in your workflow by increasing the maximum number of cores to be used at one time with the <code>--cores</code> command</li> </ul> <pre><code>snakemake --cores 4 --software-deployment-method conda\n</code></pre>"},{"location":"workshop_material/03_create_a_basic_workflow/#our-final-snakemake-workflow","title":"Our final snakemake workflow!","text":"<p>See basic_demo_workflow for the final Snakemake workflow we've created up to this point</p> <p>Back to homepage</p>"},{"location":"workshop_material/04_leveling_up_your_workflow/","title":"04 - Leveling up your workflow!","text":""},{"location":"workshop_material/04_leveling_up_your_workflow/#04-leveling-up-your-workflow","title":"04 - Leveling up your workflow!","text":""},{"location":"workshop_material/04_leveling_up_your_workflow/#catching-up","title":"Catching up","text":"<p>From section 03, you should have the following Snakefile:</p> <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n    shell:\n        \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n    log:\n        \"logs/trim_galore/{sample}.log\"\n    conda:\n        \"envs/trimgalore.yaml\"\n    threads: 2\n    shell:\n        \"trim_galore {input} -o ../results/trimmed/ --paired --cores {threads} &amp;&gt; {log}\"\n</code></pre> <p></p>"},{"location":"workshop_material/04_leveling_up_your_workflow/#41-use-a-profile-for-hpc","title":"4.1 Use a profile for HPC","text":"<p>In section 3.16, we have seen that a snakemake workflow can be run on an HPC cluster. To reduce the boilerplate, we can use a configuration profile to configure default options. In this case, we use it to set the <code>--executor</code> and the <code>--jobs</code> options. Default resources for all rules should be set using the <code>default-resources</code> option.</p> <p>Make a <code>slurm</code> profile folder</p> <pre><code># create the profile folder inside demo_workflow/workflow\ncd demo_workflow/workflow\nmkdir slurm\ntouch slurm/config.yaml\n</code></pre> <p>write the following to config.yaml</p> <pre><code>jobs: 20\nexecutor: slurm\ndefault-resources:\n    runtime: 10\n    mem_mb: 512\n    cpus_per_task: 8\n</code></pre> <p>Then run the snakemake workflow using the <code>slurm</code> profile</p> <p></p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun/run again\nsnakemake --dryrun --workflow-profile slurm --software-deployment-method conda\n</code></pre> <pre><code>snakemake --workflow-profile slurm --software-deployment-method conda\n</code></pre><p></p> <p>Possible error</p> <p>You may get errors like this: </p><pre><code>Error executing rule trim_galore on cluster (jobid: 6, external: 25266818, jobscript: /mnt/ufs18/home-081/fullarda/CMSE_890-602_snakemake/demo_workflow/workflow/.snakemake/tmp.yebsz1sm/snakejob.trim_galore.6.sh). For error details see the cluster log and the log files of the involved rule(s).\n</code></pre> The \"cluster log\" is the SLURM output log of the job. If you read the SLURM logs, it may show errors like this: <pre><code>MissingOutputException in rule trim_galore in file /mnt/ufs18/home-081/fullarda/CMSE_890-602_snakemake/demo_workflow/workflow/Snakefile, line 49:\nJob 0 completed successfully, but some output files are missing. Missing files after 5 seconds. This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.\n</code></pre><p></p> <p>To resolve this, add <code>--latency-wait 30</code> to increase the wait time to 30s. This value can be increased further if needed.</p> <p>If you interrupt the execution of a snakemake workflow using CTRL+C, the SLURM executor plugin will automatically cancel the jobs.</p> <p>You can specify different resources (memory, cpus, gpus, etc.) for each rule in the workflow.</p> <p>Here we give more CPU resources to <code>trim_galore</code> to make it run faster.</p> Edit snakefile <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n    shell:\n        \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n    log:\n        \"logs/trim_galore/{sample}.log\"\n    conda:\n        \"envs/trimgalore.yaml\"\n    threads: 2\n+   resources:\n+       cpus_per_task=8\n    shell:\n        \"trim_galore {input} -o ../results/trimmed/ --paired --cores {threads} &amp;&gt; {log}\"\n</code></pre> Current snakefile: <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n    shell:\n        \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n    log:\n        \"logs/trim_galore/{sample}.log\"\n    conda:\n        \"envs/trimgalore.yaml\"\n    threads: 2\n    resources:\n        cpus_per_task=8\n    shell:\n        \"trim_galore {input} -o ../results/trimmed/ --paired --cores {threads} &amp;&gt; {log}\"\n</code></pre> <p></p> <p>Run the workflow again</p> <p></p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun/run again\nsnakemake --dryrun --workflow-profile slurm --software-deployment-method conda\n</code></pre> <pre><code>snakemake --workflow-profile slurm --software-deployment-method conda\n</code></pre><p></p> <ul> <li>If you monitor the progress of your jobs using <code>squeue --me -o \"%.7i %9P %35j %.8u %.2t %.12M %.12L %.5C %.7m %.4D %R\"</code>, you will notice that some jobs now request 2 or 8 CPUs.</li> </ul> output <pre><code>JOBID         USER     ACCOUNT   NAME        CPUS MIN_MEM PARTITI START_TIME     TIME_LEFT STATE    NODELIST(REASON)\n26763281      lkemp    nesi99991 spawner-jupy   4      4G interac 2022-05-11T1     7:21:18 RUNNING  wbn003\n26763492      lkemp    nesi99991 snakejob.fas   2    512M large   2022-05-11T1        9:59 RUNNING  wbn144\n26763493      lkemp    nesi99991 snakejob.tri   8    512M large   2022-05-11T1        9:59 RUNNING  wbn212\n26763494      lkemp    nesi99991 snakejob.fas   2    512M large   2022-05-11T1        9:59 RUNNING  wbn145\n26763495      lkemp    nesi99991 snakejob.fas   2    512M large   2022-05-11T1        9:59 RUNNING  wbn146\n26763496      lkemp    nesi99991 snakejob.tri   8    512M large   2022-05-11T1        9:59 RUNNING  wbn217\n26763497      lkemp    nesi99991 snakejob.tri   8    512M large   2022-05-11T1        9:59 RUNNING  wbn229\n</code></pre> <p></p>"},{"location":"workshop_material/04_leveling_up_your_workflow/#logging","title":"Logging","text":"<p>SLURM log files for each rule can be found in the workflow's <code>.snakemake/slurm_logs</code> directory. The SLURM logs are stored separately for each rule within a directory named after the rule.</p>"},{"location":"workshop_material/04_leveling_up_your_workflow/#snakemake-slurm-communication","title":"Snakemake-SLURM communication","text":"<p>The SLURM executor plugin tracks job completion, failure and cancellations. You can request specific numbers of retries in the case of job failures with the option <code>--retries=N</code> where <code>N</code> is the number of retries per rule.</p> <p>Alternatively, you can use SLURM itself to handle the requeuing of jobs by adding <code>--slurm-requeue</code> to the <code>snakemake</code> command or in the workflow profile.</p> <p>Once all of this is in place, we can:</p> <ul> <li>submit Slurm jobs with the right resources per Snakemake rule,</li> <li>cancel the workflow and Slurms jobs using CTRL-C,</li> <li>keep all slurm jobs log files in a dedicated folder.</li> </ul> <p>Exercise</p> <p>Run the snakemake workflow with Slurm jobs then use <code>scancel JOBID</code> to cancel some Slurm. See how Snakemake reacts.</p>"},{"location":"workshop_material/04_leveling_up_your_workflow/#42-pull-out-parameters","title":"4.2 Pull out parameters","text":"<p>We can set parameters for commands using the <code>params</code> rule option so that they can be more human-readable:</p> Edit snakefile <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n    shell:\n        \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n+   params:\n+       \"--paired\"\n    log:\n        \"logs/trim_galore/{sample}.log\"\n    conda:\n        \"envs/trimgalore.yaml\"\n    threads: 2\n    resources:\n        cpus_per_task=8\n    shell:\n-       \"trim_galore {input} -o ../results/trimmed/ --paired --cores {threads} &amp;&gt; {log}\"\n+       \"trim_galore {input} -o ../results/trimmed/ {params} --cores {threads} &amp;&gt; {log}\"\n</code></pre> Current snakefile: <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n    shell:\n        \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n    params:\n        \"--paired\"\n    log:\n        \"logs/trim_galore/{sample}.log\"\n    conda:\n        \"envs/trimgalore.yaml\"\n    threads: 2\n    resources:\n        cpus_per_task=8\n    shell:\n        \"trim_galore {input} -o ../results/trimmed/ {params} --cores {threads} &amp;&gt; {log}\"\n</code></pre> <p></p> <p>Run a dryrun to check it works</p> <p></p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun again\nsnakemake --dryrun --workflow-profile slurm --software-deployment-method conda\n</code></pre><p></p>"},{"location":"workshop_material/04_leveling_up_your_workflow/#43-pull-out-user-configurable-options","title":"4.3 Pull out user configurable options","text":"<p>We can separate the user configurable options away from the workflow. This supports reproducibility by minimising the chance the user makes changes to the core workflow.</p> <p>Create a configuration file in a new directory <code>config/</code></p> <p>File structure:</p> <pre><code>demo_workflow/\n      |_______results/\n      |_______workflow/\n      |          |_______logs/\n      |          |_______slurm/\n      |          |_______Snakefile\n      |          |_______status.py\n      |_______config\n                 |_______config.yaml\n</code></pre> <p>code</p> <p></p><pre><code># create config directory\nmkdir ../config\n</code></pre> <pre><code># create configuration file\ntouch ../config/config.yaml\n</code></pre><p></p> <p>Now we need to pull out the parameters the user would likely need to configure. Let's give the user the option to pass any parameters they like to fastqc. In our <code>../config/config.yaml</code> file, add the configuration options and add a couple flags to be passed to fastqc and multiqc:</p> <pre><code># set software parameters for...\nPARAMS:\n  # ... fastqc\n  FASTQC: \"--kmers 5\"\n  # ... multiqc\n  MULTIQC: \"--flat\"\n</code></pre> Edit snakefile : In the Snakefile, tell Snakemake to grab the variables <code>PARAMS</code> from <code>../config/config.yaml</code> <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n+   params:\n+       fastqc_params = config['PARAMS']['FASTQC']\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n-       \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} &amp;&gt; {log}\"\n+       \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} {params.fastqc_params} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n+   params:\n+       multiqc_params = config['PARAMS']['MULTIQC']\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n    shell:\n-       \"multiqc {input} -o ../results/ &amp;&gt; {log}\"\n+       \"multiqc {input} -o ../results/ {params.multiqc_params} &amp;&gt; {log}\"\n\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n    params:\n        \"--paired\"\n    log:\n        \"logs/trim_galore/{sample}.log\"\n    conda:\n        \"envs/trimgalore.yaml\"\n    threads: 2\n    resources:\n        cpus_per_task=8\n    shell:\n        \"trim_galore {input} -o ../results/trimmed/ {params} --cores {threads} &amp;&gt; {log}\"\n</code></pre> Current snakefile <pre><code># define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    params:\n        fastqc_params = config['PARAMS']['FASTQC']\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} {params.fastqc_params} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    params:\n        multiqc_params = config['PARAMS']['MULTIQC']\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n    shell:\n        \"multiqc {input} -o ../results/ {params.multiqc_params} &amp;&gt; {log}\"\n\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n    params:\n        \"--paired\"\n    log:\n        \"logs/trim_galore/{sample}.log\"\n    conda:\n        \"envs/trimgalore.yaml\"\n    threads: 2\n    resources:\n        cpus_per_task=8\n    shell:\n        \"trim_galore {input} -o ../results/trimmed/ {params} --cores {threads} &amp;&gt; {log}\"\n</code></pre> <p></p> <p>Let's use our configuration file! Run workflow again:</p> <p></p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun/run again\nsnakemake --dryrun --workflow-profile slurm --software-deployment-method conda\n</code></pre> <pre><code>snakemake --workflow-profile slurm --software-deployment-method conda\n</code></pre><p></p> Didn't work? My error: <pre><code>KeyError in line 19 of /scale_wlg_persistent/filesets/project/nesi99991/snakemake20220512/lkemp/snakemake_workshop/demo_workflow/workflow/Snakefile:\n'PARAMS'\n  File \"/scale_wlg_persistent/filesets/project/nesi99991/snakemake20220512/lkemp/snakemake_workshop/demo_workflow/workflow/Snakefile\", line 19, in &lt;module&gt;\n</code></pre> <p></p> <p>Snakemake can't find our 'Key' - we haven't told Snakemake where our config file is so it can't find our config variables. We can do this by passing the location of our config file to the <code>--configfile</code> flag</p> <p>code</p> <p></p><pre><code># remove output of last run\nrm -r ../results/\n</code></pre> <pre><code># run dryrun/run again\n- snakemake --dryrun --workflow-profile slurm --software-deployment-method conda\n- snakemake --workflow-profile slurm --software-deployment-method conda\n+ snakemake --dryrun --workflow-profile slurm --software-deployment-method conda --configfile ../config/config.yaml\n+ snakemake --workflow-profile slurm --software-deployment-method conda --configfile ../config/config.yaml\n</code></pre><p></p> <p>Alternatively, we can define our config file in our Snakefile in a situation where the configuration file is likely to always be named the same and be in the exact same location <code>../config/config.yaml</code> and you don't need the flexibility for the user to specify their own configuration files:</p> Edit snakefile <pre><code># define our configuration file\n+ configfile: \"../config/config.yaml\"\n\n# define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    params:\n        fastqc_params = config['PARAMS']['FASTQC']\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} {params.fastqc_params} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    params:\n        multiqc_params = config['PARAMS']['MULTIQC']\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n    shell:\n        \"multiqc {input} -o ../results/ {params.multiqc_params} &amp;&gt; {log}\"\n\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n    params:\n        \"--paired\"\n    log:\n        \"logs/trim_galore/{sample}.log\"\n    conda:\n        \"envs/trimgalore.yaml\"\n    threads: 2\n    resources:\n        cpus_per_task=8\n    shell:\n        \"trim_galore {input} -o ../results/trimmed/ {params} --cores {threads} &amp;&gt; {log}\"\n</code></pre> Current snakefile: <pre><code># define our configuration file\nconfigfile: \"../config/config.yaml\"\n\n# define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    params:\n        fastqc_params = config['PARAMS']['FASTQC']\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} {params.fastqc_params} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    params:\n        multiqc_params = config['PARAMS']['MULTIQC']\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n    shell:\n        \"multiqc {input} -o ../results/ {params.multiqc_params} &amp;&gt; {log}\"\n\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n    params:\n        \"--paired\"\n    log:\n        \"logs/trim_galore/{sample}.log\"\n    conda:\n        \"envs/trimgalore.yaml\"\n    threads: 2\n    resources:\n        cpus_per_task=8\n    shell:\n        \"trim_galore {input} -o ../results/trimmed/ {params} --cores {threads} &amp;&gt; {log}\"\n</code></pre> <p></p> <p>Then we don't need to specify where the configuration file is on the command line</p> <p>code</p> <p></p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun/run again\n- snakemake --dryrun --workflow-profile slurm --software-deployment-method conda --configfile ../config/config.yaml\n- snakemake --workflow-profile slurm --software-deployment-method conda --configfile ../config/config.yaml\n+ snakemake --dryrun --workflow-profile slurm --software-deployment-method conda\n+ snakemake --workflow-profile slurm --software-deployment-method conda\n</code></pre><p></p>"},{"location":"workshop_material/04_leveling_up_your_workflow/#44-leave-messages-for-the-user","title":"4.4 Leave messages for the user","text":"<p>We can provide the user of our workflow more information on what is happening at each stage/rule of our workflow via the <code>message:</code> directive. We are able to call many variables such as:</p> <ul> <li>Input and output files <code>{input}</code> and <code>{output}</code></li> <li>Specific input and output files such as <code>{input.R1}</code></li> <li>Our <code>{params}</code>, <code>{log}</code> and <code>{threads}</code> directives</li> </ul> Edit snakefile <pre><code># define our configuration file\nconfigfile: \"../config/config.yaml\"\n\n# define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    params:\n        fastqc_params = config['PARAMS']['FASTQC']\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n+   message:\n+       \"Undertaking quality control checks {input}\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} {params.fastqc_params} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    params:\n        multiqc_params = config['PARAMS']['MULTIQC']\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n+   message:\n+       \"Compiling a HTML report for quality control checks. Writing to {output}.\"\n    shell:\n        \"multiqc {input} -o ../results/ {params.multiqc_params} &amp;&gt; {log}\"\n\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n    params:\n        \"--paired\"\n    log:\n        \"logs/trim_galore/{sample}.log\"\n    conda:\n        \"envs/trimgalore.yaml\"\n    threads: 2\n    resources:\n        cpus_per_task=8\n+   message:\n+       \"Trimming using these parameter: {params}. Writing logs to {log}. Using {threads} threads.\"\n    shell:\n        \"trim_galore {input} -o ../results/trimmed/ {params} --cores {threads} &amp;&gt; {log}\"\n</code></pre> Current snakefile: <pre><code># define our configuration file\nconfigfile: \"../config/config.yaml\"\n\n# define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    params:\n        fastqc_params = config['PARAMS']['FASTQC']\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    message:\n        \"Undertaking quality control checks {input}\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} {params.fastqc_params} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    params:\n        multiqc_params = config['PARAMS']['MULTIQC']\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n    message:\n        \"Compiling a HTML report for quality control checks. Writing to {output}.\"\n    shell:\n        \"multiqc {input} -o ../results/ {params.multiqc_params} &amp;&gt; {log}\"\n\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n    params:\n        \"--paired\"\n    log:\n        \"logs/trim_galore/{sample}.log\"\n    conda:\n        \"envs/trimgalore.yaml\"\n    threads: 2\n    resources:\n        cpus_per_task=8\n    message:\n        \"Trimming using these parameter: {params}. Writing logs to {log}. Using {threads} threads.\"\n    shell:\n        \"trim_galore {input} -o ../results/trimmed/ {params} --cores {threads} &amp;&gt; {log}\"\n</code></pre> <p></p> <p>code</p> <pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun/run again\nsnakemake --dryrun --workflow-profile slurm --software-deployment-method conda\nsnakemake --workflow-profile slurm --software-deployment-method conda\n</code></pre> <ul> <li>Now our messages are printed to the screen as our workflow runs</li> </ul> output <pre><code>Building DAG of jobs...\nUsing shell: /usr/bin/bash\nProvided cluster nodes: 20\nJob stats:\njob            count    min threads    max threads\n-----------  -------  -------------  -------------\nall                1              1              1\nfastqc             3              2              2\nmultiqc            1              1              1\ntrim_galore        3              2              2\ntotal              8              1              2\n\nSelect jobs to execute...\n\n[Wed May 11 13:20:52 2022]\nJob 4: Undertaking quality control checks ../../data/NA24694_1.fastq.gz ../../data/NA24694_2.fastq.gz\n\nSubmitted job 4 with external jobid '26763840'.\n\n[Wed May 11 13:20:52 2022]\nJob 6: Trimming using these parameter: --paired. Writing logs to logs/trim_galore/NA24695.log. Using 2 threads.\n\nSubmitted job 6 with external jobid '26763841'.\n\n[Wed May 11 13:20:52 2022]\nJob 2: Undertaking quality control checks ../../data/NA24631_1.fastq.gz ../../data/NA24631_2.fastq.gz\n\nSubmitted job 2 with external jobid '26763842'.\n\n[Wed May 11 13:20:52 2022]\nJob 3: Undertaking quality control checks ../../data/NA24695_1.fastq.gz ../../data/NA24695_2.fastq.gz\n\nSubmitted job 3 with external jobid '26763843'.\n\n[Wed May 11 13:20:52 2022]\nJob 5: Trimming using these parameter: --paired. Writing logs to logs/trim_galore/NA24631.log. Using 2 threads.\n\nSubmitted job 5 with external jobid '26763844'.\n\n[Wed May 11 13:20:52 2022]\nJob 7: Trimming using these parameter: --paired. Writing logs to logs/trim_galore/NA24694.log. Using 2 threads.\n\nSubmitted job 7 with external jobid '26763845'.\n[Wed May 11 13:22:08 2022]\nFinished job 4.\n1 of 8 steps (12%) done\n[Wed May 11 13:22:08 2022]\nFinished job 6.\n2 of 8 steps (25%) done\n[Wed May 11 13:22:08 2022]\nFinished job 2.\n3 of 8 steps (38%) done\n[Wed May 11 13:22:08 2022]\nFinished job 3.\n4 of 8 steps (50%) done\nSelect jobs to execute...\n\n[Wed May 11 13:22:09 2022]\nJob 1: Compiling a HTML report for quality control checks. Writing to ../results/multiqc_report.html.\n\nSubmitted job 1 with external jobid '26763848'.\n[Wed May 11 13:22:09 2022]\nFinished job 5.\n5 of 8 steps (62%) done\n[Wed May 11 13:22:09 2022]\nFinished job 7.\n6 of 8 steps (75%) done\n[Wed May 11 13:24:21 2022]\nFinished job 1.\n7 of 8 steps (88%) done\nSelect jobs to execute...\n\n[Wed May 11 13:24:21 2022]\nlocalrule all:\n    input: ../results/multiqc_report.html, ../results/trimmed/NA24631_1_val_1.fq.gz, ../results/trimmed/NA24695_1_val_1.fq.gz, ../results/trimmed/NA24694_1_val_1.fq.gz, ../results/trimmed/NA24631_2_val_2.fq.gz, ../results/trimmed/NA24695_2_val_2.fq.gz, ../results/trimmed/NA24694_2_val_2.fq.gz\n    jobid: 0\n    resources: mem_mb=512, disk_mb=1000, tmpdir=/dev/shm/jobs/26763281, cpus=2, time_min=10\n\n[Wed May 11 13:24:21 2022]\nFinished job 0.\n8 of 8 steps (100%) done\nComplete log: .snakemake/log/2022-05-11T132052.454902.snakemake.log\n</code></pre> <p></p>"},{"location":"workshop_material/04_leveling_up_your_workflow/#45-create-temporary-files","title":"4.5 Create temporary files","text":"<p>In our workflow, we are likely to be creating files that we don't want, but are used or produced by our workflow (intermediate files). We can mark such files as temporary so Snakemake will remove the file once it doesn't need to use it anymore.</p> <p>For example, we might not want to keep our fastqc output files since our multiqc report merges all of our fastqc reports for each sample into one report. Let's have a look at the files currently produced by our workflow with:</p> <p>code</p> <pre><code>ls -lh ../results/fastqc/\n</code></pre> output <pre><code>total 4.5M\n-rw-rw----+ 1 lkemp nesi99991 250K May 11 13:22 NA24631_1_fastqc.html\n-rw-rw----+ 1 lkemp nesi99991 327K May 11 13:22 NA24631_1_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 249K May 11 13:22 NA24631_2_fastqc.html\n-rw-rw----+ 1 lkemp nesi99991 327K May 11 13:22 NA24631_2_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 254K May 11 13:22 NA24694_1_fastqc.html\n-rw-rw----+ 1 lkemp nesi99991 334K May 11 13:22 NA24694_1_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 250K May 11 13:22 NA24694_2_fastqc.html\n-rw-rw----+ 1 lkemp nesi99991 328K May 11 13:22 NA24694_2_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 252K May 11 13:22 NA24695_1_fastqc.html\n-rw-rw----+ 1 lkemp nesi99991 328K May 11 13:22 NA24695_1_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 253K May 11 13:22 NA24695_2_fastqc.html\n-rw-rw----+ 1 lkemp nesi99991 330K May 11 13:22 NA24695_2_fastqc.zip\n</code></pre> <p></p> <p>Let's mark all the trimmed fastq files as temporary in our Snakefile by wrapping it up in the <code>temp()</code> function</p> Edit snakefile <pre><code># define our configuration file\nconfigfile: \"../config/config.yaml\"\n\n# define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n-       html = [\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"],\n+       html = temp([\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"]),\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    params:\n        fastqc_params = config['PARAMS']['FASTQC']\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    message:\n        \"Undertaking quality control checks {input}\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} {params.fastqc_params} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    params:\n        multiqc_params = config['PARAMS']['MULTIQC']\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n    message:\n        \"Compiling a HTML report for quality control checks. Writing to {output}.\"\n    shell:\n        \"multiqc {input} -o ../results/ {params.multiqc_params} &amp;&gt; {log}\"\n\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n    params:\n        \"--paired\"\n    log:\n        \"logs/trim_galore/{sample}.log\"\n    conda:\n        \"envs/trimgalore.yaml\"\n    threads: 2\n    resources:\n        cpus_per_task=8\n    message:\n        \"Trimming using these parameter: {params}. Writing logs to {log}. Using {threads} threads.\"\n    shell:\n        \"trim_galore {input} -o ../results/trimmed/ {params} --cores {threads} &amp;&gt; {log}\"\n</code></pre> Current snakefile: <pre><code># define our configuration file\nconfigfile: \"../config/config.yaml\"\n\n# define samples from data directory using wildcards\nSAMPLES, = glob_wildcards(\"../../data/{sample}_1.fastq.gz\")\n\n# target OUTPUT files for the whole workflow\nrule all:\n    input:\n        \"../results/multiqc_report.html\",\n        expand([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"], sample = SAMPLES)\n\n# workflow\nrule fastqc:\n    input:\n        R1 = \"../../data/{sample}_1.fastq.gz\",\n        R2 = \"../../data/{sample}_2.fastq.gz\"\n    output:\n        html = temp([\"../results/fastqc/{sample}_1_fastqc.html\", \"../results/fastqc/{sample}_2_fastqc.html\"]),\n        zip = [\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"]\n    params:\n        fastqc_params = config['PARAMS']['FASTQC']\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 2\n    conda:\n        \"envs/fastqc.yaml\"\n    message:\n        \"Undertaking quality control checks {input}\"\n    shell:\n        \"fastqc {input.R1} {input.R2} -o ../results/fastqc/ -t {threads} {params.fastqc_params} &amp;&gt; {log}\"\n\nrule multiqc:\n    input:\n        expand([\"../results/fastqc/{sample}_1_fastqc.zip\", \"../results/fastqc/{sample}_2_fastqc.zip\"], sample = SAMPLES)\n    output:\n        \"../results/multiqc_report.html\"\n    params:\n        multiqc_params = config['PARAMS']['MULTIQC']\n    log:\n        \"logs/multiqc/multiqc.log\"\n    conda:\n        \"envs/multiqc.yaml\"\n    message:\n        \"Compiling a HTML report for quality control checks. Writing to {output}.\"\n    shell:\n        \"multiqc {input} -o ../results/ {params.multiqc_params} &amp;&gt; {log}\"\n\nrule trim_galore:\n    input:\n        [\"../../data/{sample}_1.fastq.gz\", \"../../data/{sample}_2.fastq.gz\"]\n    output:\n        [\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"]\n    params:\n        \"--paired\"\n    log:\n        \"logs/trim_galore/{sample}.log\"\n    conda:\n        \"envs/trimgalore.yaml\"\n    threads: 2\n    resources:\n        cpus_per_task=8\n    message:\n        \"Trimming using these parameter: {params}. Writing logs to {log}. Using {threads} threads.\"\n    shell:\n        \"trim_galore {input} -o ../results/trimmed/ {params} --cores {threads} &amp;&gt; {log}\"\n</code></pre> <p></p> <p>code</p> <p></p><pre><code># remove output of last run\nrm -r ../results/*\n</code></pre> <pre><code># run dryrun/run again\nsnakemake --dryrun --workflow-profile slurm --software-deployment-method conda\nsnakemake --workflow-profile slurm --software-deployment-method conda\n</code></pre><p></p> <p>Now when we have a look at the <code>../results/fastqc/</code> directory with:</p> <pre><code>ls -lh ../results/fastqc/\n</code></pre> <ul> <li>These html files have been removed once Snakemake no longer needs the files for another rule/operation, and we've saved some space on our computer (from 4.5 megabytes to 3 megabytes in this directory).</li> </ul> output <pre><code>total 3.0M\n-rw-rw----+ 1 lkemp nesi99991 327K May 11 13:26 NA24631_1_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 327K May 11 13:26 NA24631_2_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 334K May 11 13:26 NA24694_1_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 328K May 11 13:26 NA24694_2_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 328K May 11 13:26 NA24695_1_fastqc.zip\n-rw-rw----+ 1 lkemp nesi99991 330K May 11 13:26 NA24695_2_fastqc.zip\n</code></pre> <p></p> <p>This becomes particularly important when our data become big data, since we don't want to keep any massive intermediate output files that we don't need. Otherwise this can start to clog up the memory on our computer. It ensures our workflow is scalable when our data becomes big data.</p>"},{"location":"workshop_material/04_leveling_up_your_workflow/#46-generating-a-snakemake-report","title":"4.6 Generating a snakemake report","text":"<p>With Snakemake, we can automatically generate detailed self-contained HTML reports after we run our workflow with the following command:</p> <p>code</p> <pre><code>snakemake --report ../results/snakemake_report.html\n</code></pre> <p>Note</p> <p>You won't be able to view a rendered version of this html while it is on the remote server, however after you transfer it to your local computer you should be able to view it in your web browser.</p> <p>In our report:</p> <ul> <li>We get an interactive version of our directed acyclic graph (DAG).</li> <li>When you click on a node in the DAG, the input and output files are fully outlined, the exact software used and the exact shell command that was run.</li> <li>You are also provided with runtime information under the <code>Statistics</code> tab outlining how long each rule/sample ran for, and the date/time each file was created.</li> </ul> <p>My report</p> <p></p> <p></p> <p>These reports are highly configurable, have a look at an example of what can be done with a report here</p> <p>See more information on creating Snakemake reports in the Snakemake documentation</p>"},{"location":"workshop_material/04_leveling_up_your_workflow/#47-linting-your-workflow","title":"4.7 Linting your workflow","text":"<p>Snakemake has a built in linter to support you building best practice workflows, let's try it out:</p> <p>code</p> <pre><code>snakemake --lint\n</code></pre> output <pre><code>Congratulations, your workflow is in a good condition!\n</code></pre> <p></p> <p>Writing a best practice workflow is more important than having Marie Kondo level tidiness, it increases the chance your workflow will continue to be used and maintained by others (and ourselves), making the code we write useful (it's exciting seeing someone else using your code!). If your workflow was used in scientific research, it makes your workflow accessible for people to reproduce your research findings; it isn't going to be a nightmare for them to run and they are more likely to try and have success doing so.</p> <p>Read more about the best practices for Snakemake</p>"},{"location":"workshop_material/04_leveling_up_your_workflow/#takeaways","title":"Takeaways","text":"<ul> <li>Pull out your parameters and put them in <code>params:</code> directive</li> <li>Pulling the user configurable options away from the core workflow will support reproducibility by reducing the chance of changes to the core workflow</li> <li>Leaving messages for the user of your workflow will help them understand what is happening at each stage and follow the workflows progress</li> <li>Mark files you won't need once the workflow completes to reduce the memory usage - particularly when dealing with big data</li> <li>Generate a snakemake report to get a summary of the workflow run - these are highly configurable</li> <li>Lint your workflow and check it complies with best practices - this supports reproducibility and portability</li> <li>There is so much more to explore, such as creating modular workflows, automatically grabbing remote files from places like Google Cloud Storage and Dropbox, run various types of scripts such as python scripts, R and RMarkdown scripts and Jupyter Notebooks</li> </ul>"},{"location":"workshop_material/04_leveling_up_your_workflow/#summary-commands","title":"Summary commands","text":"<ul> <li>Use the parameter directive (<code>params</code>) to keep the parameters and flags of your programs separate from your shell command, for example:</li> </ul> <pre><code>params:\n    \"--paired\"\n</code></pre> <ul> <li>Run your snakemake workflow (using environment modules to load your software AND with a configuration file) with:</li> </ul> <pre><code>snakemake --cores 2 --software-deployment-method conda --configfile ../config/config.yaml\n</code></pre> <ul> <li>Alternatively, define your config file in the Snakefile:</li> </ul> <pre><code>configfile: \"../config/config.yaml\"\n</code></pre> <ul> <li>Use the <code>message</code> directive to provide information to the user on what is happening real time, for example:</li> </ul> <pre><code>message:\n    \"Undertaking quality control checks {input}\"\n</code></pre> <ul> <li>Mark temporary files to remove (once they are no longer needed by the workflow) with <code>temp()</code>, for example:</li> </ul> <pre><code>temp([\"../results/trimmed/{sample}_1_val_1.fq.gz\", \"../results/trimmed/{sample}_2_val_2.fq.gz\"])\n</code></pre> <ul> <li>Create a basic interactive Snakemake report after running your workflow with:</li> </ul> <pre><code>snakemake --report ../results/snakemake_report.html\n</code></pre>"},{"location":"workshop_material/04_leveling_up_your_workflow/#our-final-snakemake-workflow","title":"Our final snakemake workflow!","text":"<p>See leveled_up_demo_workflow for the final Snakemake workflow we've created up to this point</p> <p>Back to homepage</p>"},{"location":"workshop_material/05_we_want_more/","title":"05 - We want more!","text":""},{"location":"workshop_material/05_we_want_more/#05-we-want-more","title":"05 - We want more!","text":"<ul> <li> <p>Before going full ham creating your own Snakemake workflow, it's a good idea to see if someone has already made what you were thinking of creating! Have a look at the workflows available in the Snakemake workflow catalog. Also see all the results that were returned by searching \"rna pipeline snakemake\" in github. If it doesn't do exactly what you want, you can fork it and adapt the workflow to your use</p> </li> <li> <p>Snakemake provides incredible documentation and there is an insane amount more you can do with Snakemake beyond what was covered here</p> </li> <li> <p>Snakemake also provides a great tutorial (and a short tutorial for those pressed for time)</p> </li> <li> <p>Here is a fully established pipeline being used in production use that extends on the workflow we created in this workshop: https://github.com/ESR-NZ/human_genomics_pipeline</p> </li> </ul> <p>Back to homepage</p>"}]}